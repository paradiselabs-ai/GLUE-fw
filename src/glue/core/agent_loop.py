"""
Agent Loop module for the GLUE framework.

"""

from typing import List, Dict, Any, Callable, Tuple, Optional, TYPE_CHECKING, Set
import json
from .working_memory import WorkingMemory, PersistentMemory
from .schemas import (
    ParseAnalyzeOutput,
    PlanPhaseOutput,
    ToolSelectionOutput,
    MemoryDecisionOutput,
    SelfEvalOutput,
    FormatResultOutput,
)
from .orchestrator_schemas import (
    ReportRecord,
)
from ..utils.json_utils import extract_json
from .types import TaskStatus, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT
import logging
import re
from enum import Enum
from pydantic import BaseModel, Field, ValidationError, model_validator, ConfigDict
from typing import Literal


class LLMReportParams(BaseModel):
    """Validate parameters generated by the LLM for report_task_completion"""

    @model_validator(mode="before")
    def normalize_status(cls, data):
        # Map status 'completed' to 'success'
        status = data.get("status")
        if status == "completed":
            data["status"] = "success"
        return data

    status: Literal["success", "failure", "escalation"] = Field(
        ..., description="Completion status from LLM"
    )
    detailed_answer: str = Field(
        ..., min_length=1, description="Detailed answer from LLM"
    )
    artifact_keys: List[str] = Field(
        default_factory=list, description="Optional artifact keys from LLM"
    )
    failure_reason: Optional[str] = Field(
        None, description="Optional failure reason from LLM"
    )
    model_config = ConfigDict(extra="ignore")


# Enum to track TeamLead orchestration workflow state
class OrchestrationState(Enum):
    PLANNING = "PLANNING"
    READY_TO_DELEGATE = "READY_TO_DELEGATE"
    WAITING_FOR_REPORTS = "WAITING_FOR_REPORTS"
    READY_TO_SYNTHESIZE = "READY_TO_SYNTHESIZE"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"


logger = logging.getLogger(__name__)

# Regex to identify delegated task IDs: e.g., "teamname-task-<8 hex chars>"
TASK_ID_REGEX = re.compile(r"^[a-z0-9\-]+\-task\-[0-9a-f]{8}$")

if TYPE_CHECKING:
    # Type checking for Team models imported above
    pass


# Orchestrator loop for team leads, handling task delegation and retry logic.
class TeamLeadAgentLoop:
    """
    Orchestrator loop for team leads, handling task delegation and retry logic.
    Minimal implementation to support retry evaluation.
    """
    def __init__(self, team, delegate_tool, agent_llm, max_retries=DEFAULT_MAX_RETRIES):
        self.team = team
        self.delegate_tool = delegate_tool
        self.agent_llm = agent_llm
        self.max_retries = max_retries
        self.terminated = False
        self.task_states = {}

    async def _evaluate_report(self, report: ReportRecord):
        """
        Evaluate a ReportRecord and update corresponding TaskRecord retry logic.
        """
        for record in self.task_states.values():
            if record.task_id == report.task_id:
                if report.status == "failure":
                    if record.retries < self.max_retries:
                        record.retries += 1
                        record.state = TaskStatus.PENDING_RETRY.value
                        self.terminated = False
                    else:
                        record.state = TaskStatus.FAILED.value
                        self.terminated = True
                elif report.status == "success":
                    record.state = TaskStatus.COMPLETE.value
                elif report.status == "escalation":
                    record.state = TaskStatus.ESCALATED.value
                    self.terminated = True
                break


# Deprecated: Use GlueSmolAgent logic for member task loops.
class TeamMemberAgentLoop:
    def __init__(self, *args, **kwargs):
        raise NotImplementedError(
            "TeamMemberAgentLoop is deprecated; use GlueSmolAgent instead."
        )


class TeamMemberAgentLoop:
    """loop for Team Member agents. Tools given are report_task_completion tool."""

    # Maximum number of times to retry LLM formatting when generating final JSON
    FORMAT_RESULT_MAX_RETRIES = 1

    def __init__(
        self,
        agent_id: str,
        team_id: str,
        report_tool: callable,
        agent_llm: Optional[Any] = None,
    ):
        self.agent_id = agent_id
        self.team_id = team_id
        # Track the current task ID explicitly to avoid stale state
        self.current_task_id = None
        # Store and wrap report_tool to auto-inject task_id from loop state
        raw_report_tool = report_tool

        def auto_report_tool(
            *,
            status,
            detailed_answer,
            artifact_keys=None,
            failure_reason=None,
            **_ignored,
        ):
            """Validate LLM-generated parameters and inject fixed context."""
            # Validate only the fields LLM is responsible for
            try:
                llm_params = LLMReportParams(
                    status=status,
                    detailed_answer=detailed_answer,
                    artifact_keys=artifact_keys or [],
                    failure_reason=failure_reason,
                )
            except ValidationError as e:
                logger.error(f"LLMReportParams validation error: {e}")
                raise
            # Call the underlying tool with injected context
            result = raw_report_tool(
                task_id=self.state["current_task_id"],
                status=llm_params.status,
                detailed_answer=llm_params.detailed_answer,
                artifact_keys=llm_params.artifact_keys,
                calling_agent_id=self.agent_id,
                calling_team=self.team_id,
            )
            return result

        self.report_tool = auto_report_tool
        # LLM interface for analysis and planning
        self.agent_llm = agent_llm
        # Registry for external tools
        self.tools: Dict[str, Callable] = {}
        # Loop termination state
        self.terminated: bool = False
        self.termination_reason: Optional[str] = None
        # Tracking for refine/revise logic
        self.last_substep_description: Optional[str] = None
        self.last_context_info: Optional[Dict[str, Any]] = None
        self.last_self_eval: Optional[SelfEvalOutput] = None
        self.last_tool_name: Optional[str] = None
        self.last_tool_params: Optional[Dict[str, Any]] = None
        self.state = {"current_task_id": None, "status": "idle"}
        # Initialize working memory and control counters
        self.working_memory = WorkingMemory()
        self.turn_counter: int = 0
        self.max_attempts: int = 3  # cap on loop iterations
        self.substeps: List[str] = []  # ordered list of substeps from plan
        self.current_step: int = 0  # index of the current substep being executed
        self.attempts_per_step: Dict[
            int, int
        ] = {}  # track number of attempts per substep

    async def start(self, fetch_task_func: callable):
        """Start the Team Member loop to perform tasks and report completion."""
        logger.debug(f"TeamMemberAgentLoop({self.agent_id}): start invoked")

        # Continuously fetch and process tasks until explicitly terminated.
        while not self.terminated:
            # Fetch next task
            task = await fetch_task_func(self.agent_id)
            # Debug log the full task received to inspect context_keys and prior data
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): received task dict: {task}"
            )
            task_id = task.get("task_id")
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): fetched task {task_id}"
            )
            logger.info(
                f"TeamMemberAgentLoop({self.agent_id}): begin processing task {task_id}"
            )

            # Initialize state for new task
            self.current_task_id = task_id
            self.state["current_task_id"] = task_id
            self.state["status"] = "working"

            # Reset execution context
            self.working_memory = WorkingMemory()
            self.turn_counter = 0
            self.substeps = []
            self.current_step = 0
            self.attempts_per_step = {}

            # Gather and analyze context
            self.gather_context(task)
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): initial context gathered for task {task_id}"
            )
            analysis = await self.parse_and_analyze(task.get("description", ""))
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): parse_and_analyze result: {analysis}"
            )
            self.working_memory.add_entry(
                self.turn_counter, str(analysis), "ParseAnalyze"
            )

            # Plan substeps
            plan_out = await self.plan_phase(self.working_memory.get_entries())
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): plan_phase result: substeps={plan_out.substeps}, tools={plan_out.tool_requirements}, confidence={plan_out.estimated_confidence}"
            )
            self.working_memory.add_entry(self.turn_counter, str(plan_out), "PlanPhase")
            self.substeps = plan_out.substeps
            self.current_step = 0
            self.attempts_per_step = {}

            # Execute substeps
            while not self.terminated:
                self.turn_counter += 1
                entries = self.working_memory.get_entries()
                recent = [e for e in entries if e.get("source_tool") != "Curated"][
                    -self.max_attempts :
                ]
                context_info = {
                    "turn": self.turn_counter,
                    "memory_entries": entries,
                    "recent_raw_outputs": recent,
                }

                # Debug log execution stats
                logger.debug(
                    f"Execution stats - turn_counter: {self.turn_counter}, current_step: {self.current_step}, attempts_per_step: {self.attempts_per_step}"
                )
                desc = self.substeps[self.current_step]
                # Info log the substep execution for tracing
                logger.info(
                    f"Task {self.current_task_id}: executing substep {self.current_step + 1}/{len(self.substeps)}: '{desc}'"
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): executing substep {self.current_step} '{desc}' with context={context_info}"
                )
                tool_name, result = await self.select_and_invoke_tool(
                    desc, context_info
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): tool '{tool_name}' returned result: {result}"
                )

                # Save and optionally curate memory
                self.working_memory.add_entry(self.turn_counter, str(result), tool_name)
                mem_dec = await self.should_save_to_memory(result)
                if mem_dec.save_to_memory:
                    self.working_memory.add_entry(
                        self.turn_counter, mem_dec.analysis or "", "Curated"
                    )

                # Self evaluation
                eval_out = await self.evaluate_self(result)
                self.last_self_eval = eval_out
                confidence = eval_out.confidence_level.lower()
                # Debug log the self-evaluation confidence
                logger.debug(f"self-evaluation confidence: {confidence}")
                self.attempts_per_step[self.current_step] = (
                    self.attempts_per_step.get(self.current_step, 0) + 1
                )

                if confidence.startswith("high"):
                    # Next substep or finish
                    if self.current_step < len(self.substeps) - 1:
                        self.current_step += 1
                        continue

                    # Report success
                    answer = await self.format_result(result)
                    status = "success" if result.get("success") else "failure"
                    logger.debug(
                        f"Preparing to report task completion for {task_id}: status={status}"
                    )
                    await self.report_tool(
                        status=status,
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                    )
                    self.terminate(f"Task {task_id} completed")
                    self.state["status"] = "completed"
                    break
                elif confidence.startswith("medium"):
                    result = await self.refine_result(result)
                    continue
                elif confidence.startswith("low"):
                    if self.attempts_per_step[self.current_step] < self.max_attempts:
                        result = await self.revise_strategy(result)
                        continue

                    # Report failure
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report failure for task {task_id}: status=failure"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} failed")
                    self.state["status"] = "completed"
                    break
                else:
                    # Critical error
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report critical error for task {task_id}"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=[],
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} critical error")
                    self.state["status"] = "completed"
                    break
        # End of persistent task loop.

    async def performTaskStub(self, task_description: str) -> dict:
        """Placeholder for performing the task (simulated)."""
        import asyncio

        # Simulate work
        await asyncio.sleep(1)
        # Hardcoded success result with placeholder artifacts
        return {"success": True, "artifacts": []}

    def generateSummaryStub(self, result: dict) -> str:
        """Placeholder for generating a detailed answer based on the result."""
        # Provide details including success status and any artifacts or errors
        if result.get("success"):
            artifacts = result.get("artifacts", [])
            return f"Task completed successfully with artifacts: {artifacts}"
        else:
            # Include any error message if present
            error_msg = result.get("error", "Unknown error")
            return f"Task failed: {error_msg}"

    def gather_context(self, task: Dict[str, Any]):
        """Collect initial context from the task description."""
        # Log context_keys and required_artifacts to verify context passing
        logger.debug(
            f"TeamMemberAgentLoop({self.agent_id}): gather_context for task {task.get('task_id')}, "
            f"description={task.get('description')}, context_keys={task.get('context_keys')}, required_artifacts={task.get('required_artifacts')}"
        )
        # Add the description to working memory
        self.working_memory.add_entry(
            self.turn_counter, task["description"], "TaskReceived"
        )

    async def parse_and_analyze(self, description: str) -> ParseAnalyzeOutput:
        """Use LLM to extract high-level requirements from task description as structured JSON."""
        # Trace parse_and_analyze invocation
        logger.info(
            f"Task {self.current_task_id}: parse_and_analyze starting for description: {description}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            keywords = description.split()[:3]
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"keywords": keywords},
            )
        # Construct prompt requesting detailed structured JSON output per schema with examples
        prompt = (
            "## ROLE & OBJECTIVE ##\n"
            "You are a Meticulous Reasoning Agent. Your objective is to deeply analyze a given task description, "
            "understand its underlying components, and structure this understanding into a precise JSON output.\n"
            "\n"
            "## TASK ##\n"
            "Carefully examine the user-provided task description. Your task is to:\n"
            "1. Decompose the input task description into its fundamental operations, inputs, outputs, and constraints.\n"
            "2. Produce a detailed, step-by-step `thought_process` string. This string must clearly articulate the logical steps you took to interpret the task, identify its core components, and decide on the structure for the `analysis`.\n"
            "3. Formulate a structured `analysis` object. This object should be a key-value mapping representing the identified elements, requirements, operations, and any constraints from the task description.\n"
            "\n"
            "## OUTPUT SPECIFICATION ##\n"
            "Your entire response **MUST** be a single, valid, fenced JSON code block. No other text, preamble, or explanation should precede or follow the JSON block.\n"
            "The JSON object **MUST** adhere to the `ParseAnalyzeOutput` schema defined below.\n"
            "\n"
            "## SCHEMA DEFINITION: ParseAnalyzeOutput ##\n"
            "```json\n"
            "{\n"
            '  "thought_process": "string",  // Detailed, human-readable explanation of the reasoning steps taken to interpret and decompose the task. This should cover how core requirements, subcomponents, and constraints were identified.\n'
            '  "analysis": {}             // A structured key-value mapping capturing the core elements, requirements, operations, inputs, outputs, and any identified constraints of the task. The specific keys within this object will depend on the task being analyzed.\n'
            "}\n"
            "```\n"
            "\n"
            "## EXAMPLES (Few-Shot Learning) ##\n"
            "\n"
            "**Example 1:**\n"
            'Input Task: "Filter a list to only include even numbers and then sort it."\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "thought_process": "1. Identify primary actions: \'filter\' and \'sort\'. 2. Determine filter criterion: \'even numbers\'. 3. Identify input type: a list. 4. Infer intermediate output: a list of even numbers. 5. Identify final output: a sorted list of even numbers. 6. Determine operation order: filter first, then sort.",\n'
            '  "analysis": {\n'
            '    "task_summary": "Filter a list for even numbers, then sort the result.",\n'
            '    "operations": [\n'
            '      {"action": "filter", "criterion": "even numbers", "applies_to": "list"},\n'
            '      {"action": "sort", "order": "ascending (default)", "applies_to": "list of even numbers"}\n'
            '    ],\n'
            '    "sequence": ["filter", "sort"],\n'
            '    "inputs": ["a list of numbers"],\n'
            '    "output": "a sorted list containing only even numbers from the input"\n'
            "  }\n"
            "}\n"
            "```\n"
            "\n"
            "**Example 2:**\n"
            'Input Task: "Compute the intersection subset of two user ID sets."\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "thought_process": "1. Identify the core operation: \'set intersection\'. 2. Identify inputs: \'two user ID sets\'. Let\'s call them Set A and Set B. 3. Understand \'subset\' in this context: it refers to the elements common to both sets. 4. Determine output: a new set containing only the user IDs present in both Set A and Set B.",\n'
            '  "analysis": {\n'
            '    "task_summary": "Find common elements between two sets of user IDs.",\n'
            '    "operation": "intersection",\n'
            '    "operand_type": "set",\n'
            '    "inputs": {\n'
            '      "set1": "first user ID set",\n'
            '      "set2": "second user ID set"\n'
            '    },\n'
            '    "output": "a new set containing user IDs common to both input sets"\n'
            "  }\n"
            "}\n"
            "```\n"
            "\n"
            "## YOUR TASK ##\n"
            "Now, apply this process to the following task description:\n"
            f'"{description}"\n'
        )
        # Debug log the constructed prompt
        logger.debug(f"parse_and_analyze prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"parse_and_analyze raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if parsed:
            try:
                return ParseAnalyzeOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return ParseAnalyzeOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"analysis": response},
            )

    async def plan_phase(self, memory: List[Dict[str, Any]]) -> PlanPhaseOutput:
        """Use LLM to break memory entries into substeps, tool requirements, and estimated confidence as structured JSON."""
        # Trace plan_phase invocation
        logger.info(
            f"Task {self.current_task_id}: plan_phase starting with memory entries count: {len(memory)}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )
        # Construct an enriched planning prompt guiding the LLM to produce clear substeps, tool requirements, and confidence
        prompt = (
            "## ROLE & OBJECTIVE ##\n"
            "You are a Strategic Planning Agent. Your primary objective is to meticulously analyze a set of task descriptions (provided as 'memory entries') "
            "and decompose them into a clear, actionable, and ordered sequence of substeps. You must also identify necessary tools and provide an overall confidence assessment for the successful execution of the plan.\n"
            "\n"
            "## INPUT DESCRIPTION ##\n"
            "You will receive 'memory entries' as a JSON array of strings. Each string in the array represents a task component or a full task description that needs to be planned.\n"
            "\n"
            "## TASK INSTRUCTIONS ##\n"
            "Based on the provided 'memory entries':\n"
            "1.  **Decompose:** Break down the overall task(s) into a logically ordered list of specific, actionable `substeps`. Each substep should represent a distinct unit of work.\n"
            "2.  **Identify Tools:** For the entire plan, list all unique `tool_requirements` needed to complete the substeps. If a tool is used in multiple substeps, list it only once.\n"
            "3.  **Assess Confidence:** Provide an `estimated_confidence` rating ('low', 'medium', or 'high') for the likelihood of successfully executing the entire plan as formulated. Consider task complexity, number of steps, and potential dependencies.\n"
            "\n"
            "## OUTPUT SPECIFICATION ##\n"
            "Your entire response **MUST** be a single, valid, fenced JSON code block. No other text, preamble, or explanation should precede or follow the JSON block.\n"
            "The JSON object **MUST** adhere to the `PlanPhaseOutput` schema defined below.\n"
            "\n"
            "## SCHEMA DEFINITION: PlanPhaseOutput ##\n"
            "```json\n"
            "{\n"
            '  "substeps": [],             // Array of strings. Each string is a concise, actionable description of a single substep in the execution plan. Steps MUST be in logical execution order.\n'
            '  "tool_requirements": [],    // Array of strings. Each string is the name of a unique tool required for executing one or more substeps in the plan. List each tool only once.\n'
            '  "estimated_confidence": ""  // String. Must be one of: "low", "medium", "high". Reflects confidence in the plan\'s successful execution.\n'
            "}\n"
            "```\n"
            "\n"
            "## EXAMPLES (Few-Shot Learning) ##\n"
            "\n"
            "**Example 1:**\n"
            'Input Memory Entries: ["Convert input text to all lowercase and then calculate the frequency of each word."]\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "substeps": ["Convert input text to lowercase", "Tokenize the lowercased text into words", "Count the frequency of each tokenized word"],\n'
            '  "tool_requirements": ["textProcessor"],\n'
            '  "estimated_confidence": "high"\n'
            "}\n"
            "```\n"
            "*Rationale for Example 1 Confidence: Straightforward text processing task with clearly defined steps, likely handled by a single robust tool.*\n"
            "\n"
            "**Example 2:**\n"
            'Input Memory Entries: ["User wants to fetch their profile data from the main API. After that, filter for users who are currently active. Finally, export the list of active user IDs and their email addresses as a CSV file."]\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "substeps": ["Authenticate with the main API", "Fetch user profile data using the API client", "Parse the API response", "Filter the parsed data to identify active users", "Extract user IDs and email addresses for active users", "Format extracted data into CSV structure", "Save the CSV data to a file named \'active_users.csv\'"],\n'
            '  "tool_requirements": ["apiClient", "dataFilter", "csvExporter"],\n'
            '  "estimated_confidence": "medium"\n'
            "}\n"
            "```\n"
            "*Rationale for Example 2 Confidence: Multi-step process involving external API interaction (potential for network issues or API changes), data transformation, and file I/O. Assumes tools are reliable but acknowledges more points of potential failure than Example 1.*\n"
            "\n"
            "## YOUR TASK ##\n"
            "Now, devise an execution plan for the following memory entries:\n"
            f"{json.dumps(memory)}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"plan_phase prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"plan_phase raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return PlanPhaseOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return PlanPhaseOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )

    async def select_and_invoke_tool(
        self, task_description: str, context_info: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Any]:
        """Use LLM to select the next tool and invoke it based on structured JSON output."""
        # Info log tool selection start
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool for substep '{task_description}'"
        )
        # Track substep and context
        self.last_substep_description = task_description
        self.last_context_info = context_info
        if not self.agent_llm:
            # Fallback stub execution
            self.last_tool_name = "performTaskStub"
            self.last_tool_params = {"task_description": task_description}
            result = await self.performTaskStub(task_description)
            return "performTaskStub", result
        # Construct enriched tool selection prompt with few-shot examples and schema guidance
        prompt = (
            "## ROLE & OBJECTIVE ##\n"
            "You are a Precise Tool Dispatcher Agent. Your primary objective is to accurately select the most appropriate tool to execute a given 'substep' and to correctly formulate the parameters required by that tool based on the provided 'current_context' and the tool's defined schema.\n"
            "\n"
            "## INPUTS ##\n"
            "1.  `substep_to_execute`: A string describing the specific action to be performed.\n"
            "2.  `available_tools_with_schemas`: A JSON array where each object describes an available tool, including its name, description, and a schema for its expected parameters.\n"
            "3.  `current_context`: A JSON object containing data relevant to the current state or task, which may be used to populate tool parameters.\n"
            "\n"
            "## TASK INSTRUCTIONS ##\n"
            "Based on the provided inputs:\n"
            "1.  **Analyze Substep:** Understand the intent and requirements of the `substep_to_execute`.\n"
            "2.  **Review Tools:** Examine the `available_tools_with_schemas`. For each tool, understand its purpose (from its description) and what parameters it accepts (from its schema).\n"
            "3.  **Select Tool:** Choose the single `selected_tool_name` from `available_tools_with_schemas` that is best suited to accomplish the `substep_to_execute`.\n"
            "4.  **Formulate Parameters:** Construct the `tool_parameters` object. The keys in this object MUST match the parameter names defined in the schema of the `selected_tool_name`. The values for these parameters should be derived from the `current_context` or the `substep_to_execute` itself, if applicable.\n"
            "\n"
            "## OUTPUT SPECIFICATION ##\n"
            "Your entire response **MUST** be a single, valid, fenced JSON code block. No other text, preamble, or explanation should precede or follow the JSON block.\n"
            "The JSON object **MUST** adhere to the `ToolSelectionOutput` schema defined below.\n"
            "\n"
            "## SCHEMA DEFINITION: ToolSelectionOutput ##\n"
            "```json\n"
            "{\n"
            '  "selected_tool_name": "string",  // The exact name of the tool chosen from available_tools_with_schemas.\n'
            '  "tool_parameters": {}            // A JSON object. Keys are parameter names for the selected tool, values are the arguments for those parameters. This object MUST match the parameter schema of the selected tool.\n'
            "}\n"
            "```\n"
            "\n"
            "## EXAMPLES (Few-Shot Learning) ##\n"
            "\n"
            "**Example 1:**\n"
            'Substep to Execute: "Parse and analyze the user\'s initial problem description."\n'
            "Available Tools and Schemas:\n"
            "```json\n"
            "[\n"
            '  {"name": "parse_and_analyze", "description": "Analyzes task descriptions.", "parameters_schema": {"description": "string"}},\n'
            '  {"name": "report_task_completion", "description": "Reports task status.", "parameters_schema": {"status": "string", "detailed_answer": "string"}}\n'
            "]\n"
            "```\n"
            'Current Context: {"user_query": "My computer is slow and I need help.", "session_id": "xyz123"}\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "selected_tool_name": "parse_and_analyze",\n'
            '  "tool_parameters": {"description": "My computer is slow and I need help."}\n'
            "}\n"
            "```\n"
            "\n"
            "**Example 2:**\n"
            'Substep to Execute: "Report to the user that the analysis is complete and provide the summary."\n'
            "Available Tools and Schemas:\n"
            "```json\n"
            "[\n"
            '  {"name": "parse_and_analyze", "description": "Analyzes task descriptions.", "parameters_schema": {"description": "string"}},\n'
            '  {"name": "report_task_completion", "description": "Reports task status.", "parameters_schema": {"status": "string", "detailed_answer": "string"}},\n'
            '  {"name": "send_email", "description": "Sends an email.", "parameters_schema": {"recipient": "string", "subject": "string", "body": "string"}}\n'
            "]\n"
            "```\n"
            'Current Context: {"analysis_summary": "The user needs system diagnostics.", "current_status": "analysis_complete"}\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "selected_tool_name": "report_task_completion",\n'
            '  "tool_parameters": {"status": "analysis_complete", "detailed_answer": "The user needs system diagnostics."}\n'
            "}\n"
            "```\n"
            "\n"
            "## YOUR TASK ##\n"
            "Substep to Execute: " + f'"{task_description}"\n'
        )
        if context_info is not None:
            prompt += f"Context: {json.dumps(context_info)}"
        # Debug log the constructed prompt
        logger.debug(f"select_and_invoke_tool prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"select_and_invoke_tool raw response:\n{response}")
        parsed = extract_json(response)
        # Info log parsed selection JSON
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool parsed JSON: {parsed}"
        )
        selection = None
        if isinstance(parsed, dict):
            try:
                selection = ToolSelectionOutput.model_validate(parsed)
                # Info log the selected tool and parameters
                logger.info(
                    f"Task {self.current_task_id}: selected tool '{selection.selected_tool_name}' with params {selection.tool_parameters}"
                )
            except Exception:
                selection = None
        # Invoke selected tool or fallback
        if selection:
            tool_name = selection.selected_tool_name
            params = selection.tool_parameters or {}
            # Track tool invocation
            self.last_tool_name = tool_name
            self.last_tool_params = params
            # Intercept internal report: use auto_report_tool to inject task_id
            if tool_name == "report_task_completion":
                # Debug log interception and params
                logger.debug(
                    f"Intercepting report_task_completion for task {self.current_task_id} with params {params}"
                )
                result = await self.report_tool(**params)
                # Debug log the result of tool invocation
                logger.debug(
                    f"select_and_invoke_tool report_task_completion result: {result}"
                )
                return tool_name, result
            # First try internal method
            if hasattr(self, tool_name) and callable(getattr(self, tool_name)):
                try:
                    result = await getattr(self, tool_name)(**params)
                    # Debug log the result of internal tool invocation
                    logger.debug(
                        f"select_and_invoke_tool internal tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
            # Then try external tool registry
            if tool_name in self.tools and callable(self.tools[tool_name]):
                try:
                    result = await self.tools[tool_name](**params)
                    # Debug log the result of external tool invocation
                    logger.debug(
                        f"select_and_invoke_tool external tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
        # Fallback to performTaskStub on failure
        result = await self.performTaskStub(task_description)
        return "performTaskStub", result

    async def should_save_to_memory(self, result: Any) -> MemoryDecisionOutput:
        """Prompt agent to decide if result should be saved using the MemoryDecisionOutput schema."""
        if not self.agent_llm:
            return MemoryDecisionOutput(save_to_memory=False)
        prompt = (
            "## ROLE & OBJECTIVE ##\n"
            "You are a Strategic Memory Curation Agent. Your objective is to analyze a given 'tool_result' and decide whether its content is valuable enough to be saved to long-term memory for future reference in subsequent tasks. Your decision should be based on the potential future utility, novelty, and significance of the information.\n"
            "\n"
            "## TASK INSTRUCTIONS ##\n"
            "You will be provided with a 'tool_result'. Evaluate this result based on the following criteria to determine if it should be saved to memory:\n"
            "1.  **Significance & Utility:** Does the result contain critical information, key findings, definitive answers, or data that is likely to be essential for understanding context or making decisions in subsequent steps or related future tasks? Avoid saving trivial, intermediate, or overly verbose data unless it has unique importance.\n"
            "2.  **Novelty & Non-Redundancy:** Is the information new and not merely a restatement of known facts or easily derivable information? Avoid saving information that is already implicitly known or will be immediately superseded.\n"
            "3.  **Conciseness & Clarity:** If the information is valuable, can it be (or is it already) represented concisely? While you don't reformat, consider if the essence is clear.\n"
            "\n"
            "Based on your evaluation:\n"
            "-   Set `save_to_memory` to `true` if the result meets the criteria for being valuable for long-term memory. Otherwise, set it to `false`.\n"
            "-   Provide a concise `analysis` string explaining your reasoning for the `save_to_memory` decision. This should briefly highlight why the information is (or is not) deemed important enough to save, referencing the criteria above.\n"
            "\n"
            "## OUTPUT SPECIFICATION ##\n"
            "Your entire response **MUST** be a single, valid, fenced JSON code block. No other text, preamble, or explanation should precede or follow the JSON block.\n"
            "The JSON object **MUST** adhere to the `MemoryDecisionOutput` schema defined below.\n"
            "\n"
            "## SCHEMA DEFINITION: MemoryDecisionOutput ##\n"
            "```json\n"
            "{\n"
            '  "save_to_memory": "boolean", // true if the tool_result should be saved to memory, false otherwise.\n'
            '  "analysis": "string"         // A concise justification for the save_to_memory decision, highlighting key factors considered (e.g., "Contains critical user preferences", "Intermediate calculation, not needed for long-term memory").\n'
            "}\n"
            "```\n"
            "\n"
            "## EXAMPLES (Few-Shot Learning) ##\n"
            "\n"
            "**Example 1: Information worth saving**\n"
            'Tool Result: {"user_id": "123", "preferences": {"theme": "dark", "notifications": "weekly"}, "last_login": "2023-10-26"}\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "save_to_memory": true,\n'
            '  "analysis": "Contains critical user preferences (theme, notifications) and last login timestamp, which are highly relevant for future interactions and personalization."\n'
            "}\n"
            "```\n"
            "\n"
            "**Example 2: Information NOT worth saving (intermediate/temporary)**\n"
            'Tool Result: {"status_code": 200, "message": "Data fetched successfully, 1024 bytes received."}\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "save_to_memory": false,\n'
            '  "analysis": "Represents a transient operational status (successful fetch). The actual data fetched would be evaluated separately if provided; this status message itself is not useful for long-term memory."\n'
            "}\n"
            "```\n"
            "\n"
            "**Example 3: Information NOT worth saving (too generic/derivable)**\n"
            'Tool Result: "The current year is 2024."\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "save_to_memory": false,\n'
            '  "analysis": "States a commonly known and easily derivable fact (current year). Not essential to store in memory."\n'
            "}\n"
            "\n"
            "**Example 4: Information worth saving (critical error/finding)**\n"
            'Tool Result: {"error_code": "API_AUTH_FAILURE", "details": "User authentication token expired or invalid for service X.", "timestamp": "2024-03-15T10:30:00Z"}\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "save_to_memory": true,\n'
            '  "analysis": "Critical API authentication failure. Saving this error detail and timestamp is important for debugging and future reference regarding system reliability."\n'
            "}\n"
            "```\n"
            "\n"
            "## YOUR TASK ##\n"
            "Decide whether to save the following tool result to memory. Provide your reasoning in the analysis.\n"
            "Tool Result:\n"
            f"{json.dumps(result, indent=2) if isinstance(result, (dict, list)) else str(result)}\n"  # Ensure complex results are nicely formatted
        )
        response = await self.agent_llm.generate(prompt)
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return MemoryDecisionOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub
        return MemoryDecisionOutput(save_to_memory=False)

    async def evaluate_self(self, result: Any) -> SelfEvalOutput:
        """Use LLM to self-evaluate and return a SelfEvalOutput object."""
        if not self.agent_llm:
            # Default stub: always high confidence for fallback
            stub = SelfEvalOutput(
                evaluation_summary="Fallback stub self-evaluation",
                consistency_check="Passed",
                alignment_check="Passed",
                confidence_level="high",
                error_detected=False,
            )
            return stub
        # Construct prompt for SelfEvalOutput
        prompt = (
            "## ROLE & OBJECTIVE ##\n"
            "You are a Meticulous Self-Evaluation Agent. Your objective is to critically assess a given 'Result' in the context of 'Working Memory Entries'. You must evaluate the result's quality, its consistency with prior information, its alignment with the original task goals (if inferable), and your confidence in its correctness, also noting any detected errors.\n"
            "\n"
            "## INPUTS ##\n"
            "1.  `Result`: The output or data generated by a previous step or tool that needs evaluation.\n"
            "2.  `Working_Memory_Entries`: A JSON array of strings or objects representing prior context, earlier results, or task goals that the current `Result` should be consistent and aligned with.\n"
            "\n"
            "## TASK INSTRUCTIONS: EVALUATION CRITERIA ##\n"
            "Carefully analyze the `Result` against the `Working_Memory_Entries`. Populate the fields of the `SelfEvalOutput` schema based on the following criteria:\n"
            "\n"
            "1.  **`evaluation_summary` (string):**\n"
            "    - Provide a concise overall summary of your assessment of the `Result`.\n"
            "    - Mention its perceived quality, relevance, and completeness in light of the context.\n"
            "    - *Example: \"Result appears to be a well-formed user profile, containing all expected fields based on the initial request in memory.\"*\n"
            "\n"
            "2.  **`consistency_check` (string - 'consistent', 'inconsistent', 'partially_consistent', 'not_applicable'):**\n"
            "    - Compare the `Result` with information in `Working_Memory_Entries`.\n"
            "    - 'consistent': Information in `Result` aligns with and does not contradict prior context.\n"
            "    - 'inconsistent': `Result` directly contradicts prior information or established facts in memory.\n"
            "    - 'partially_consistent': Some aspects align, others conflict or are new and unevaluated against memory.\n"
            "    - 'not_applicable': No relevant prior context in memory to check consistency against.\n"
            "    - *Briefly justify your choice if not 'consistent' or 'not_applicable'.*\n"
            "\n"
            "3.  **`alignment_check` (string - 'aligned', 'misaligned', 'partially_aligned', 'not_applicable'):**\n"
            "    - Assess if the `Result` contributes to or addresses the original task goals or user requests, if such goals are present or inferable from `Working_Memory_Entries`.\n"
            "    - 'aligned': `Result` directly helps achieve the inferred task goals.\n"
            "    - 'misaligned': `Result` seems irrelevant or counter-productive to the inferred task goals.\n"
            "    - 'partially_aligned': `Result` is somewhat relevant but doesn't fully address the goals, or addresses a sub-part.\n"
            "    - 'not_applicable': Task goals are not clear from memory, or the result is too atomic to judge alignment independently.\n"
            "    - *Briefly justify your choice if not 'aligned' or 'not_applicable'.*\n"
            "\n"
            "4.  **`confidence_level` (string - 'high', 'medium', 'low'):**\n"
            "    - Your overall confidence in the correctness, quality, and appropriateness of the `Result` given the context.\n"
            "    - 'high': Result seems accurate, complete, and well-suited.\n"
            "    - 'medium': Minor uncertainties, potential incompleteness, or some unverified assumptions.\n"
            "    - 'low': Significant doubts about accuracy, completeness, or appropriateness; potential errors suspected.\n"
            "\n"
            "5.  **`error_detected` (boolean):**\n"
            "    - Set to `true` if you identify any clear factual errors, logical flaws, formatting issues (if critical), or significant omissions in the `Result`. Otherwise, set to `false`.\n"
            "    - *If true, ensure your `evaluation_summary` or other check fields reflect the nature of the error.*\n"
            "\n"
            "## OUTPUT SPECIFICATION ##\n"
            "Your entire response **MUST** be a single, valid, fenced JSON code block. No other text, preamble, or explanation should precede or follow the JSON block.\n"
            "The JSON object **MUST** adhere to the `SelfEvalOutput` schema.\n"
            "\n"
            "## SCHEMA DEFINITION: SelfEvalOutput ##\n"
            "```json\n"
            "{\n"
            '  "evaluation_summary": "string",  // Concise overall assessment of the result.\n'
            '  "consistency_check": "string",   // One of: "consistent", "inconsistent", "partially_consistent", "not_applicable". Justify if not "consistent"/"not_applicable".\n'
            '  "alignment_check": "string",     // One of: "aligned", "misaligned", "partially_aligned", "not_applicable". Justify if not "aligned"/"not_applicable".\n'
            '  "confidence_level": "string",    // One of: "high", "medium", "low".\n'
            '  "error_detected": "boolean"      // true if a clear error is found in the result, false otherwise.\n'
            "}\n"
            "```\n"
            "\n"
            "## EXAMPLES (Few-Shot Learning) ##\n"
            "\n"
            "**Example 1:**\n"
            'Result: {"user_id": "jane.doe@example.com", "status": "active", "department": "Sales"}\n'
            'Working Memory Entries: [{"task_goal": "Retrieve active user profile for jane.doe@example.com"}, {"schema_expected": ["user_id", "status", "department", "location"]}]\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "evaluation_summary": "Result provides key user profile details for jane.doe. However, the \'location\' field, expected from memory, is missing.",\n'
            '  "consistency_check": "consistent",\n'
            '  "alignment_check": "partially_aligned",\n'
            '  "confidence_level": "medium",\n'
            '  "error_detected": true\n'
            "}\n"
            "```\n"
            "\n"
            "**Example 2:**\n"
            'Result: "The total order value is $150.75."\n'
            'Working Memory Entries: [{"action": "calculate_order_total", "items": [{"item_id": "A1", "price": 50.25, "qty": 2}, {"item_id": "B2", "price": 49.25, "qty": 1}], "previous_subtotal_calculation": 150.75}]\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "evaluation_summary": "The result correctly states the order total, matching a previous subtotal calculation found in memory.",\n'
            '  "consistency_check": "consistent",\n'
            '  "alignment_check": "aligned",\n'
            '  "confidence_level": "high",\n'
            '  "error_detected": false\n'
            "}\n"
            "```\n"
            "\n"
            "**Example 3:**\n"
            'Result: {"data_summary": "Processed 1000 records, 5 errors found."}\n'
            'Working Memory Entries: [{"task_goal": "Generate a detailed error report for all failed records."}]\n'
            "Output JSON:\n"
            "```json\n"
            "{\n"
            '  "evaluation_summary": "Result provides a high-level summary of processing, but does not fulfill the task goal of a detailed error report.",\n'
            '  "consistency_check": "not_applicable",\n'
            '  "alignment_check": "misaligned",\n'
            '  "confidence_level": "low",\n'
            '  "error_detected": false\n'
            "}\n"
            "```\n"
            "\n"
            "## YOUR TASK ##\n"
            "Evaluate the following `Result` based on the `Working_Memory_Entries` and provide your assessment in the specified JSON format.\n"
            f"Result:\n{json.dumps(result, indent=2) if isinstance(result, (dict, list)) else str(result)}\n"
            f"Working Memory Entries:\n{json.dumps(self.working_memory.get_entries(), indent=2)}\n" # Assuming get_entries() returns a list of serializable items
        )
        # Debug log the constructed prompt
        logger.debug(f"evaluate_self prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"evaluate_self raw response:\n{response}")
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return SelfEvalOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub if parsing failed
        stub = SelfEvalOutput(
            evaluation_summary="Fallback stub self-evaluation",
            consistency_check="Passed",
            alignment_check="Passed",
            confidence_level="HighConfidence",
            error_detected=False,
        )
        return stub

    async def refine_result(self, last_result: Any) -> Any:
        """Refine the last result by re-invoking the substep tool with feedback."""
        # Gather feedback from last self-evaluation
        feedback = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the feedback used for refinement
        logger.debug(f"refine_result feedback: {feedback}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["refine_feedback"] = feedback
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"refine_result context: {context}")
        # Re-invoke tool for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from refine_result
        logger.debug(f"refine_result new result: {new_result}")
        return new_result

    async def revise_strategy(self, last_result: Any) -> Any:
        """Revise strategy for the current substep by planning a new approach."""
        # Gather failure reason from last self-evaluation
        reason = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the failure reason for revision
        logger.debug(f"revise_strategy failure reason: {reason}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["failure_reason"] = reason
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"revise_strategy context: {context}")
        # Re-invoke planning/execution for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from revise_strategy
        logger.debug(f"revise_strategy new result: {new_result}")
        return new_result

    async def format_result(self, result: Any) -> str:
        """Format the final result using the internal summary stub and working memory context."""
        # Gather all memory entries
        entries = self.working_memory.get_entries()
        # Use internal summary stub for final answer
        summary = self.generateSummaryStub(result)
        # Build supporting context from entries
        context = [str(e) for e in entries]
        # Create output object
        fmt = FormatResultOutput(final_answer=summary, supporting_context=context)
        return fmt.model_dump_json()

    def register_tool(self, name: str, func: Callable):
        """Register an external tool callable under the given name."""
        self.tools[name] = func

    def terminate(self, reason: Optional[str] = None):
        """Externally terminate the loop execution."""
        self.terminated = True
        self.termination_reason = reason
