"""
Agent Loop module for the GLUE framework.

"""

from typing import List, Dict, Any, Callable, Tuple, Optional, TYPE_CHECKING, Set
import json
from .working_memory import WorkingMemory, PersistentMemory
from .schemas import (
    ParseAnalyzeOutput,
    PlanPhaseOutput,
    ToolSelectionOutput,
    MemoryDecisionOutput,
    SelfEvalOutput,
    FormatResultOutput,
)
from .orchestrator_schemas import (
    Subtask,
    ReportRecord,
    EvaluateDecision,
    FinalResult,
    TaskRecord,
)
from .teams import GlueTeam as Team
from ..utils.json_utils import extract_json
import asyncio
from .types import TaskStatus, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT
import logging
from datetime import datetime
import re
from enum import Enum
from pydantic import BaseModel, Field, ValidationError, model_validator, ConfigDict
from typing import Literal


class LLMReportParams(BaseModel):
    """Validate parameters generated by the LLM for report_task_completion"""

    @model_validator(mode="before")
    def normalize_status(cls, data):
        # Map status 'completed' to 'success'
        status = data.get("status")
        if status == "completed":
            data["status"] = "success"
        return data

    status: Literal["success", "failure", "escalation"] = Field(
        ..., description="Completion status from LLM"
    )
    detailed_answer: str = Field(
        ..., min_length=1, description="Detailed answer from LLM"
    )
    artifact_keys: List[str] = Field(
        default_factory=list, description="Optional artifact keys from LLM"
    )
    failure_reason: Optional[str] = Field(
        None, description="Optional failure reason from LLM"
    )
    model_config = ConfigDict(extra="ignore")


# Enum to track TeamLead orchestration workflow state
class OrchestrationState(Enum):
    PLANNING = "PLANNING"
    READY_TO_DELEGATE = "READY_TO_DELEGATE"
    WAITING_FOR_REPORTS = "WAITING_FOR_REPORTS"
    READY_TO_SYNTHESIZE = "READY_TO_SYNTHESIZE"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"


logger = logging.getLogger(__name__)

# Regex to identify delegated task IDs: e.g., "teamname-task-<8 hex chars>"
TASK_ID_REGEX = re.compile(r"^[a-z0-9\-]+\-task\-[0-9a-f]{8}$")

if TYPE_CHECKING:
    # Type checking for Team models imported above
    pass


class TeamMemberAgentLoop:
    """loop for Team Member agents. Tools given are report_task_completion tool."""

    # Maximum number of times to retry LLM formatting when generating final JSON
    FORMAT_RESULT_MAX_RETRIES = 1

    def __init__(
        self,
        agent_id: str,
        team_id: str,
        report_tool: callable,
        agent_llm: Optional[Any] = None,
    ):
        self.agent_id = agent_id
        self.team_id = team_id
        # Track the current task ID explicitly to avoid stale state
        self.current_task_id = None
        # Store and wrap report_tool to auto-inject task_id from loop state
        raw_report_tool = report_tool

        def auto_report_tool(
            *,
            status,
            detailed_answer,
            artifact_keys=None,
            failure_reason=None,
            **_ignored,
        ):
            """Validate LLM-generated parameters and inject fixed context."""
            # Validate only the fields LLM is responsible for
            try:
                llm_params = LLMReportParams(
                    status=status,
                    detailed_answer=detailed_answer,
                    artifact_keys=artifact_keys or [],
                    failure_reason=failure_reason,
                )
            except ValidationError as e:
                logger.error(f"LLMReportParams validation error: {e}")
                raise
            # Call the underlying tool with injected context
            result = raw_report_tool(
                task_id=self.state["current_task_id"],
                status=llm_params.status,
                detailed_answer=llm_params.detailed_answer,
                artifact_keys=llm_params.artifact_keys,
                calling_agent_id=self.agent_id,
                calling_team=self.team_id,
            )
            return result

        self.report_tool = auto_report_tool
        # LLM interface for analysis and planning
        self.agent_llm = agent_llm
        # Registry for external tools
        self.tools: Dict[str, Callable] = {}
        # Loop termination state
        self.terminated: bool = False
        self.termination_reason: Optional[str] = None
        # Tracking for refine/revise logic
        self.last_substep_description: Optional[str] = None
        self.last_context_info: Optional[Dict[str, Any]] = None
        self.last_self_eval: Optional[SelfEvalOutput] = None
        self.last_tool_name: Optional[str] = None
        self.last_tool_params: Optional[Dict[str, Any]] = None
        self.state = {"current_task_id": None, "status": "idle"}
        # Initialize working memory and control counters
        self.working_memory = WorkingMemory()
        self.turn_counter: int = 0
        self.max_attempts: int = 3  # cap on loop iterations
        self.substeps: List[str] = []  # ordered list of substeps from plan
        self.current_step: int = 0  # index of the current substep being executed
        self.attempts_per_step: Dict[
            int, int
        ] = {}  # track number of attempts per substep

    async def start(self, fetch_task_func: callable):
        """Start the Team Member loop to perform tasks and report completion."""
        logger.debug(f"TeamMemberAgentLoop({self.agent_id}): start invoked")

        # Continuously fetch and process tasks until explicitly terminated.
        while not self.terminated:
            # Fetch next task
            task = await fetch_task_func(self.agent_id)
            # Debug log the full task received to inspect context_keys and prior data
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): received task dict: {task}"
            )
            task_id = task.get("task_id")
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): fetched task {task_id}"
            )
            logger.info(
                f"TeamMemberAgentLoop({self.agent_id}): begin processing task {task_id}"
            )

            # Initialize state for new task
            self.current_task_id = task_id
            self.state["current_task_id"] = task_id
            self.state["status"] = "working"

            # Reset execution context
            self.working_memory = WorkingMemory()
            self.turn_counter = 0
            self.substeps = []
            self.current_step = 0
            self.attempts_per_step = {}

            # Gather and analyze context
            self.gather_context(task)
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): initial context gathered for task {task_id}"
            )
            analysis = await self.parse_and_analyze(task.get("description", ""))
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): parse_and_analyze result: {analysis}"
            )
            self.working_memory.add_entry(
                self.turn_counter, str(analysis), "ParseAnalyze"
            )

            # Plan substeps
            plan_out = await self.plan_phase(self.working_memory.get_entries())
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): plan_phase result: substeps={plan_out.substeps}, tools={plan_out.tool_requirements}, confidence={plan_out.estimated_confidence}"
            )
            self.working_memory.add_entry(self.turn_counter, str(plan_out), "PlanPhase")
            self.substeps = plan_out.substeps
            self.current_step = 0
            self.attempts_per_step = {}

            # Execute substeps
            while not self.terminated:
                self.turn_counter += 1
                entries = self.working_memory.get_entries()
                recent = [e for e in entries if e.get("source_tool") != "Curated"][
                    -self.max_attempts :
                ]
                context_info = {
                    "turn": self.turn_counter,
                    "memory_entries": entries,
                    "recent_raw_outputs": recent,
                }

                # Debug log execution stats
                logger.debug(
                    f"Execution stats - turn_counter: {self.turn_counter}, current_step: {self.current_step}, attempts_per_step: {self.attempts_per_step}"
                )
                desc = self.substeps[self.current_step]
                # Info log the substep execution for tracing
                logger.info(
                    f"Task {self.current_task_id}: executing substep {self.current_step + 1}/{len(self.substeps)}: '{desc}'"
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): executing substep {self.current_step} '{desc}' with context={context_info}"
                )
                tool_name, result = await self.select_and_invoke_tool(
                    desc, context_info
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): tool '{tool_name}' returned result: {result}"
                )

                # Save and optionally curate memory
                self.working_memory.add_entry(self.turn_counter, str(result), tool_name)
                mem_dec = await self.should_save_to_memory(result)
                if mem_dec.save_to_memory:
                    self.working_memory.add_entry(
                        self.turn_counter, mem_dec.analysis or "", "Curated"
                    )

                # Self evaluation
                eval_out = await self.evaluate_self(result)
                self.last_self_eval = eval_out
                confidence = eval_out.confidence_level.lower()
                # Debug log the self-evaluation confidence
                logger.debug(f"self-evaluation confidence: {confidence}")
                self.attempts_per_step[self.current_step] = (
                    self.attempts_per_step.get(self.current_step, 0) + 1
                )

                if confidence.startswith("high"):
                    # Next substep or finish
                    if self.current_step < len(self.substeps) - 1:
                        self.current_step += 1
                        continue

                    # Report success
                    answer = await self.format_result(result)
                    status = "success" if result.get("success") else "failure"
                    logger.debug(
                        f"Preparing to report task completion for {task_id}: status={status}"
                    )
                    await self.report_tool(
                        status=status,
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                    )
                    self.terminate(f"Task {task_id} completed")
                    self.state["status"] = "completed"
                    break
                elif confidence.startswith("medium"):
                    result = await self.refine_result(result)
                    continue
                elif confidence.startswith("low"):
                    if self.attempts_per_step[self.current_step] < self.max_attempts:
                        result = await self.revise_strategy(result)
                        continue

                    # Report failure
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report failure for task {task_id}: status=failure"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} failed")
                    self.state["status"] = "completed"
                    break
                else:
                    # Critical error
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report critical error for task {task_id}"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=[],
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} critical error")
                    self.state["status"] = "completed"
                    break
        # End of persistent task loop.

    async def performTaskStub(self, task_description: str) -> dict:
        """Placeholder for performing the task (simulated)."""
        import asyncio

        # Simulate work
        await asyncio.sleep(1)
        # Hardcoded success result with placeholder artifacts
        return {"success": True, "artifacts": []}

    def generateSummaryStub(self, result: dict) -> str:
        """Placeholder for generating a detailed answer based on the result."""
        # Provide details including success status and any artifacts or errors
        if result.get("success"):
            artifacts = result.get("artifacts", [])
            return f"Task completed successfully with artifacts: {artifacts}"
        else:
            # Include any error message if present
            error_msg = result.get("error", "Unknown error")
            return f"Task failed: {error_msg}"

    def gather_context(self, task: Dict[str, Any]):
        """Collect initial context from the task description."""
        # Log context_keys and required_artifacts to verify context passing
        logger.debug(
            f"TeamMemberAgentLoop({self.agent_id}): gather_context for task {task.get('task_id')}, "
            f"description={task.get('description')}, context_keys={task.get('context_keys')}, required_artifacts={task.get('required_artifacts')}"
        )
        # Add the description to working memory
        self.working_memory.add_entry(
            self.turn_counter, task["description"], "TaskReceived"
        )

    async def parse_and_analyze(self, description: str) -> ParseAnalyzeOutput:
        """Use LLM to extract high-level requirements from task description as structured JSON."""
        # Trace parse_and_analyze invocation
        logger.info(
            f"Task {self.current_task_id}: parse_and_analyze starting for description: {description}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            keywords = description.split()[:3]
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"keywords": keywords},
            )
        # Construct prompt requesting detailed structured JSON output per schema with examples
        prompt = (
            "Phase: Parse and Analyze Task\n"
            "You are a reasoning agent tasked with deeply analyzing the following task description. "
            "Produce a detailed, step-by-step `thought_process` explaining how you interpreted and decomposed the task, and a structured `analysis` object capturing core requirements, subcomponents, and any constraints. "
            "Return ONLY a fenced JSON object matching the ParseAnalyzeOutput schema with fields:\n"
            "- thought_process (string): detailed reasoning steps\n"
            "- analysis (object): key-value mapping for identified elements or requirements\n"
            "\n"
            "Few-shot examples:\n"
            'Input: "Filter a list to only include even numbers and then sort it."\n'
            "Output:\n```json\n"
            "{\n"
            '  "thought_process": "First identify the actions (filter then sort), determine filter criterion (even numbers), apply filter, then apply sort.",\n'
            '  "analysis": {\n'
            '    "steps": ["filter even numbers", "sort list"],\n'
            '    "filter_criterion": "even numbers",\n'
            '    "operation_order": ["filter", "sort"]\n'
            "  }\n"
            "}\n```\n"
            'Input: "Compute the intersection subset of two user ID sets."\n'
            "Output:\n```json\n"
            "{\n"
            '  "thought_process": "Recognize the need for a set intersection, identify both input sets, and compute their common elements. The term \'subset\' indicates selecting elements shared by both.",\n'
            '  "analysis": {\n'
            '    "operation": "intersection",\n'
            '    "inputs": ["set A", "set B"],\n'
            '    "subset_criteria": "common elements"\n'
            "  }\n"
            "}\n```\n"
            "Now analyze the following task description:\n"
            f"{description}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"parse_and_analyze prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"parse_and_analyze raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if parsed:
            try:
                return ParseAnalyzeOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return ParseAnalyzeOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"analysis": response},
            )

    async def plan_phase(self, memory: List[Dict[str, Any]]) -> PlanPhaseOutput:
        """Use LLM to break memory entries into substeps, tool requirements, and estimated confidence as structured JSON."""
        # Trace plan_phase invocation
        logger.info(
            f"Task {self.current_task_id}: plan_phase starting with memory entries count: {len(memory)}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )
        # Construct an enriched planning prompt guiding the LLM to produce clear substeps, tool requirements, and confidence
        prompt = (
            "Phase: Plan Task Execution\n"
            "You are a planning agent. Given the following memory entries, break down the upcoming task into an ordered list of actionable substeps. "
            "Specify any required tools for each step and provide an overall confidence rating (low, medium, high). "
            "Return ONLY a fenced JSON object matching the PlanPhaseOutput schema with keys:\n"
            "- substeps: array of step descriptions\n"
            "- tool_requirements: array of tool names needed\n"
            "- estimated_confidence: confidence level as 'low', 'medium', or 'high'\n"
            "\n"
            "Few-shot examples:\n"
            'Input memory entries: ["Convert text to lowercase and count word frequency."]\n'
            "Output:\n```json\n"
            '{"substeps":["lowercase text","count word frequencies"],"tool_requirements":["textProcessor"],"estimated_confidence":"high"}\n'
            "```\n"
            'Input memory entries: ["Fetch user data from API, filter active users, then export as CSV."]\n'
            "Output:\n```json\n"
            '{"substeps":["fetch user data via API","filter active users","export data to CSV"],"tool_requirements":["apiClient","csvExporter"],"estimated_confidence":"medium"}\n'
            "```\n"
            "Now plan execution for the following memory entries:\n"
            f"{json.dumps(memory)}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"plan_phase prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"plan_phase raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return PlanPhaseOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return PlanPhaseOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )

    async def select_and_invoke_tool(
        self, task_description: str, context_info: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Any]:
        """Use LLM to select the next tool and invoke it based on structured JSON output."""
        # Info log tool selection start
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool for substep '{task_description}'"
        )
        # Track substep and context
        self.last_substep_description = task_description
        self.last_context_info = context_info
        if not self.agent_llm:
            # Fallback stub execution
            self.last_tool_name = "performTaskStub"
            self.last_tool_params = {"task_description": task_description}
            result = await self.performTaskStub(task_description)
            return "performTaskStub", result
        # Construct enriched tool selection prompt with few-shot examples and schema guidance
        prompt = (
            "Phase: Select and Invoke Tool\n"
            "You are an intelligent agent responsible for selecting the correct tool for the following substep. "
            "Refer to the available tools and their expected parameter schemas. "
            "Return ONLY a JSON object matching the ToolSelectionOutput schema with keys:\n"
            "- selected_tool_name (string): the tool name to invoke\n"
            "- tool_parameters (object): key-value mapping of parameters for the selected tool\n"
            "\n"
            "Few-shot examples:\n"
            "Example 1:\n"
            'Substep: "Parse and analyze description"\n'
            'Context: {"description": "..."}\n'
            "Output:\n```json\n"
            '{"selected_tool_name":"parse_and_analyze","tool_parameters":{"description":"..."}}\n'
            "```\n"
            "Example 2:\n"
            'Substep: "Report task completion"\n'
            'Context: {"status":"success","detailed_answer":"..."}\n'
            "Output:\n```json\n"
            '{"selected_tool_name":"report_task_completion","tool_parameters":{"status":"success","detailed_answer":"..."}}\n'
            "```\n"
            f"Substep: {task_description}\n"
        )
        if context_info is not None:
            prompt += f"Context: {json.dumps(context_info)}"
        # Debug log the constructed prompt
        logger.debug(f"select_and_invoke_tool prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"select_and_invoke_tool raw response:\n{response}")
        parsed = extract_json(response)
        # Info log parsed selection JSON
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool parsed JSON: {parsed}"
        )
        selection = None
        if isinstance(parsed, dict):
            try:
                selection = ToolSelectionOutput.model_validate(parsed)
                # Info log the selected tool and parameters
                logger.info(
                    f"Task {self.current_task_id}: selected tool '{selection.selected_tool_name}' with params {selection.tool_parameters}"
                )
            except Exception:
                selection = None
        # Invoke selected tool or fallback
        if selection:
            tool_name = selection.selected_tool_name
            params = selection.tool_parameters or {}
            # Track tool invocation
            self.last_tool_name = tool_name
            self.last_tool_params = params
            # Intercept internal report: use auto_report_tool to inject task_id
            if tool_name == "report_task_completion":
                # Debug log interception and params
                logger.debug(
                    f"Intercepting report_task_completion for task {self.current_task_id} with params {params}"
                )
                result = await self.report_tool(**params)
                # Debug log the result of tool invocation
                logger.debug(
                    f"select_and_invoke_tool report_task_completion result: {result}"
                )
                return tool_name, result
            # First try internal method
            if hasattr(self, tool_name) and callable(getattr(self, tool_name)):
                try:
                    result = await getattr(self, tool_name)(**params)
                    # Debug log the result of internal tool invocation
                    logger.debug(
                        f"select_and_invoke_tool internal tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
            # Then try external tool registry
            if tool_name in self.tools and callable(self.tools[tool_name]):
                try:
                    result = await self.tools[tool_name](**params)
                    # Debug log the result of external tool invocation
                    logger.debug(
                        f"select_and_invoke_tool external tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
        # Fallback to performTaskStub on failure
        result = await self.performTaskStub(task_description)
        return "performTaskStub", result

    async def should_save_to_memory(self, result: Any) -> MemoryDecisionOutput:
        """Prompt agent to decide if result should be saved using the MemoryDecisionOutput schema."""
        if not self.agent_llm:
            return MemoryDecisionOutput(save_to_memory=False)
        prompt = (
            "Provide ONLY a fenced JSON response matching the MemoryDecisionOutput schema,\n"
            "with fields:\n"
            "- save_to_memory (boolean)\n"
            "- analysis (string)\n"
            "Example:\n```json\n"
            '{"save_to_memory":true,"analysis":"key details"}\n'
            "```\n"
            "Decide whether to save the following tool result:\n"
            f"{result}\n"
        )
        response = await self.agent_llm.generate(prompt)
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return MemoryDecisionOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub
        return MemoryDecisionOutput(save_to_memory=False)

    async def evaluate_self(self, result: Any) -> SelfEvalOutput:
        """Use LLM to self-evaluate and return a SelfEvalOutput object."""
        if not self.agent_llm:
            # Default stub: always high confidence for fallback
            stub = SelfEvalOutput(
                evaluation_summary="Fallback stub self-evaluation",
                consistency_check="Passed",
                alignment_check="Passed",
                confidence_level="high",
                error_detected=False,
            )
            return stub
        # Construct prompt for SelfEvalOutput
        prompt = (
            "Please output ONLY valid JSON matching the SelfEvalOutput schema "
            "(fields: evaluation_summary, consistency_check, alignment_check, confidence_level, error_detected) "
            "for the following result and context entries:\n"
            f"Result: {result}\n"
            f"Context: {json.dumps(self.working_memory.get_entries())}"
        )
        # Debug log the constructed prompt
        logger.debug(f"evaluate_self prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"evaluate_self raw response:\n{response}")
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return SelfEvalOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub if parsing failed
        stub = SelfEvalOutput(
            evaluation_summary="Fallback stub self-evaluation",
            consistency_check="Passed",
            alignment_check="Passed",
            confidence_level="HighConfidence",
            error_detected=False,
        )
        return stub

    async def refine_result(self, last_result: Any) -> Any:
        """Refine the last result by re-invoking the substep tool with feedback."""
        # Gather feedback from last self-evaluation
        feedback = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the feedback used for refinement
        logger.debug(f"refine_result feedback: {feedback}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["refine_feedback"] = feedback
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"refine_result context: {context}")
        # Re-invoke tool for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from refine_result
        logger.debug(f"refine_result new result: {new_result}")
        return new_result

    async def revise_strategy(self, last_result: Any) -> Any:
        """Revise strategy for the current substep by planning a new approach."""
        # Gather failure reason from last self-evaluation
        reason = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the failure reason for revision
        logger.debug(f"revise_strategy failure reason: {reason}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["failure_reason"] = reason
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"revise_strategy context: {context}")
        # Re-invoke planning/execution for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from revise_strategy
        logger.debug(f"revise_strategy new result: {new_result}")
        return new_result

    async def format_result(self, result: Any) -> str:
        """Format the final result using the internal summary stub and working memory context."""
        # Gather all memory entries
        entries = self.working_memory.get_entries()
        # Use internal summary stub for final answer
        summary = self.generateSummaryStub(result)
        # Build supporting context from entries
        context = [str(e) for e in entries]
        # Create output object
        fmt = FormatResultOutput(final_answer=summary, supporting_context=context)
        return fmt.model_dump_json()

    def register_tool(self, name: str, func: Callable):
        """Register an external tool callable under the given name."""
        self.tools[name] = func

    def terminate(self, reason: Optional[str] = None):
        """Externally terminate the loop execution."""
        self.terminated = True
        self.termination_reason = reason


class TeamLeadAgentLoop:
    """Full orchestrator loop for Team Lead agents."""

    def __init__(
        self, team: Team, delegate_tool: Callable, agent_llm: Optional[Any] = None
    ):
        self.team = team
        self.delegate_tool = delegate_tool
        self.agent_llm = agent_llm
        self.agent_id = team.config.lead
        self.state = {"parent_task_id": None, "status": "idle"}
        self.working_memory = WorkingMemory()
        self.persistent_memory = PersistentMemory(self.team.name)
        # Track each subtask with a structured Pydantic model
        self.task_states: Dict[str, TaskRecord] = {}
        self.terminated = False
        self.termination_reason: Optional[str] = None
        # Configure maximum orchestration retries per subtask
        self.max_retries = int(DEFAULT_MAX_RETRIES)
        self.timeout = DEFAULT_TIMEOUT  # NEW: set default timeout
        # Initialize orchestration state and tracking structures
        self.orchestration_state: OrchestrationState = OrchestrationState.PLANNING
        self.subtasks: Dict[str, Subtask] = {}
        self.task_id_map: Dict[str, str] = {}
        self.completed_subtasks: Set[str] = set()
        self.planned: bool = False
        # Track which agent each subtask is delegated to
        self.subtask_agent_map: Dict[str, str] = {}

    async def _decompose_goal(
        self, parent_task_id: str, goal_description: str
    ) -> List[Subtask]:
        """Use LLM to decompose goal into subtasks and log the JSON to working_memory."""
        if not self.agent_llm:
            raise RuntimeError("TeamLeadAgentLoop: Missing LLM for decomposition")
        members_list = ", ".join(self.team.config.members)
        prompt = (
            "Phase: Decompose Goal\n"
            "You are the Team Lead orchestrator. Given a high-level goal, break it into an ordered list of subtasks. "
            f"The available agents are: {members_list}. "
            "You may optionally specify an `assigned_agent_id` for each subtask from this list. "
            "Output ONLY a JSON array of objects, each with fields:\n"
            "- id (string): unique subtask identifier\n"
            "- description (string): subtask description\n"
            "- dependencies (array of strings): list of prerequisite subtask IDs (empty list if none)\n"
            "- assigned_agent_id (string, optional): specific agent ID to assign this subtask to (must be one of the available agents)\n"
            "Return the array fenced in ```json ... ```.\n"
            f"High-level goal: {goal_description}\n"
        )
        try:
            response = await self.agent_llm.generate(prompt)
            parsed = extract_json(response)
            if not parsed or not isinstance(parsed, list):
                raise ValueError(
                    f"Failed to parse subtasks from LLM response: {response}"
                )
            subtasks = [Subtask.model_validate(item) for item in parsed]
        except Exception as e:
            logger.error(f"TeamLeadAgentLoop._decompose_goal: {e}")
            # Abort orchestration on decomposition failure
            raise RuntimeError(f"Decomposition error: {e}")
        # Record decomposition output as structured dicts
        parsed_subtasks = [s.model_dump() for s in subtasks]
        decomposition_json = {
            "tool_name": "decompose_goal",
            "arguments": {
                "subtasks": parsed_subtasks,
                "parent_task_id": parent_task_id,
            },
        }
        self.working_memory.add_entry(
            1, json.dumps(decomposition_json), "OrchestratorOutput"
        )
        return subtasks

    async def _delegate_pending_subtasks(self) -> None:
        """Delegate any subtasks in PENDING or PENDING_RETRY state."""
        members = self.team.config.members
        if not members:
            logger.error("No members available to delegate pending subtasks")
            for sub_id, info in self.task_states.items():
                if info.get("state") in (TaskStatus.PENDING, TaskStatus.PENDING_RETRY):
                    info["state"] = TaskStatus.NO_MEMBER
            self.terminated = True
            return
        for sub_id, info in list(self.task_states.items()):
            # Treat PENDING_RETRY same as PENDING for re-delegation
            if info.get("state") in (
                TaskStatus.PENDING.value,
                TaskStatus.PENDING_RETRY.value,
            ):
                # Strict assignment logic: explicit vs. default for pending tasks
                subtask = self.subtasks.get(sub_id)
                assigned_id = subtask.assigned_agent_id if subtask else None
                if assigned_id:
                    if assigned_id in members:
                        target = assigned_id
                    else:
                        logger.error(
                            f"Invalid assigned_agent_id {assigned_id!r} for subtask {sub_id}"
                        )
                        info["state"] = TaskStatus.FAILED.value
                        continue
                else:
                    # Choose agent for initial or retried tasks
                    if info.get("state") == TaskStatus.PENDING_RETRY.value:
                        # Exclude previous agent if possible
                        prev = self.subtask_agent_map.get(sub_id)
                        logger.info(
                            f"Retrying subtask {sub_id} attempt {info['retries']}/{self.max_retries}, previous agent={prev}"
                        )
                        candidates = [m for m in members if m != prev]
                        if candidates:
                            # count active assignments among candidates
                            counts = {m: 0 for m in candidates}
                            for s, agent in self.subtask_agent_map.items():
                                rec = self.task_states.get(s)
                                if (
                                    rec
                                    and rec.state == TaskStatus.ASSIGNED.value
                                    and agent in counts
                                ):
                                    counts[agent] += 1
                            min_count = min(counts.values())
                            for m in candidates:
                                if counts[m] == min_count:
                                    target = m
                                    break
                        else:
                            target = prev
                    else:
                        # initial delegation: pick least busy
                        target = self._select_least_busy_agent()
                    if not target:
                        # No agents available; will retry later
                        continue
                # Record delegation agent
                self.subtask_agent_map[sub_id] = target
                delegate_json = {
                    "tool_name": "delegate_task",
                    "arguments": {
                        "target_agent_id": target,
                        "parent_task_id": self.state["parent_task_id"],
                        "task_description": info.get("description"),
                        "context_keys": [],
                        "required_artifacts": [],
                    },
                }
                self.working_memory.add_entry(
                    0, json.dumps(delegate_json), "OrchestratorOutput"
                )
                result = await self.delegate_tool(
                    target_agent_id=target,
                    task_description=info.get("description"),
                    parent_task_id=self.state["parent_task_id"],
                    calling_agent_id=self.agent_id,
                    calling_team=self.team.name,
                )
                info["task_id"] = result.get("task", {}).get("task_id")
                self.task_id_map[sub_id] = info["task_id"]
                info["state"] = TaskStatus.ASSIGNED.value
                info["timestamp"] = datetime.utcnow()

    async def _delegate_ready_subtasks(self) -> None:
        """Delegate subtasks whose dependencies are met and are pending."""
        members = self.team.config.members
        if not members:
            logger.error("No members available to delegate ready subtasks")
            return
        for sub_id, subtask in self.subtasks.items():
            state_info = self.task_states.get(sub_id)
            # Treat PENDING_RETRY same as PENDING for re-delegation
            if (
                state_info
                and state_info.state
                in (TaskStatus.PENDING.value, TaskStatus.PENDING_RETRY.value)
                and all(
                    dep in self.completed_subtasks for dep in state_info.dependencies
                )
            ):
                # Strict assignment logic: explicit vs. default
                assigned_id = subtask.assigned_agent_id
                if assigned_id:
                    # Validate specified agent
                    if assigned_id in members:
                        target = assigned_id
                    else:
                        logger.error(
                            f"Invalid assigned_agent_id {assigned_id!r} for subtask {sub_id}"
                        )
                        state_info.state = TaskStatus.FAILED.value
                        continue
                else:
                    # Choose agent for unassigned or retried subtasks
                    if state_info.state == TaskStatus.PENDING_RETRY.value:
                        # On retry, exclude previous agent if possible
                        prev = self.subtask_agent_map.get(sub_id)
                        candidates = [m for m in members if m != prev]
                        logger.info(
                            f"Retrying subtask {sub_id} attempt {state_info.retries}/{self.max_retries}, previous agent={prev}"
                        )
                        if candidates:
                            # count current assignments among candidates
                            counts = {m: 0 for m in candidates}
                            for s, agent in self.subtask_agent_map.items():
                                rec = self.task_states.get(s)
                                if (
                                    rec
                                    and rec.state == TaskStatus.ASSIGNED.value
                                    and agent in counts
                                ):
                                    counts[agent] += 1
                            min_count = min(counts.values())
                            # pick least busy among candidates
                            for m in candidates:
                                if counts[m] == min_count:
                                    target = m
                                    break
                        else:
                            target = prev
                    else:
                        # initial delegation: choose least-busy agent
                        target = self._select_least_busy_agent()
                    if not target:
                        # No available agents to assign now; retry later
                        continue
                # Record which agent will handle this subtask
                self.subtask_agent_map[sub_id] = target
                delegate_json = {
                    "tool_name": "delegate_task",
                    "arguments": {
                        "target_agent_id": target,
                        "parent_task_id": self.state["parent_task_id"],
                        "task_description": subtask.description,
                        "context_keys": [],
                        "required_artifacts": [],
                    },
                }
                self.working_memory.add_entry(
                    0, json.dumps(delegate_json), "OrchestratorOutput"
                )
                result = await self.delegate_tool(
                    target_agent_id=target,
                    task_description=subtask.description,
                    parent_task_id=self.state["parent_task_id"],
                    calling_agent_id=self.agent_id,
                    calling_team=self.team.name,
                )
                task_id = result.get("task", {}).get("task_id")
                self.task_id_map[sub_id] = task_id
                state_info.task_id = task_id
                state_info.state = TaskStatus.ASSIGNED.value
                state_info.timestamp = datetime.utcnow().isoformat()

    async def _wait_for_report(self, task_id: str) -> None:
        """Wait for a single subtask completion report and evaluate it."""
        while True:
            entry = self.team.shared_results.get(task_id)
            if isinstance(entry, dict) and "completion" in entry:
                report = ReportRecord.model_validate(entry["completion"])
                await self._evaluate_report(report)
                break
            await asyncio.sleep(1)

    async def _evaluate_report(self, report: ReportRecord) -> None:
        """Evaluate a single report: log decision, update task_states, and handle retry or escalation."""
        # Map report status to orchestration action
        action_map = {
            "success": "mark_complete",
            "failure": "retry",
            "escalation": "escalate",
        }
        # Include failure_reason if provided, otherwise use detailed_answer
        detail_text = (
            report.failure_reason if report.failure_reason else report.detailed_answer
        )
        decision = EvaluateDecision(
            task_id=report.task_id,
            action=action_map.get(report.status),
            details=detail_text,
        )
        # Log evaluation JSON
        eval_json = {"tool_name": "evaluate_result", "arguments": decision.model_dump()}
        self.working_memory.add_entry(0, json.dumps(eval_json), "EvaluateResult")
        # Find corresponding task state
        entry = next(
            (
                record
                for record in self.task_states.values()
                if record.task_id == report.task_id
            ),
            None,
        )
        if not entry or entry.state in (
            TaskStatus.COMPLETE.value,
            TaskStatus.FAILED.value,
        ):
            return
        entry.state = TaskStatus.REPORTED.value
        # Execute action
        if decision.action == "mark_complete":
            entry.state = TaskStatus.COMPLETE.value
        elif decision.action == "retry":
            if entry.retries < self.max_retries:
                entry.retries += 1
                entry.state = TaskStatus.PENDING_RETRY.value
            else:
                entry.state = TaskStatus.FAILED.value
                self.terminated = True
        else:  # escalate
            entry.state = TaskStatus.FAILED.value
            self.terminated = True

    async def _synthesize_final_result(self) -> str:
        """Synthesize final result via LLM or fallback and return FinalResult JSON."""
        # Update status
        self.state["status"] = "synthesizing"
        # Gather context contents
        contexts = [e["content"] for e in self.working_memory.get_entries()]
        # Pre-check: ensure at least one subtask completed successfully
        successful = [
            rec
            for rec in self.task_states.values()
            if rec.state == TaskStatus.COMPLETE.value
        ]
        if not successful:
            # No successful subtasks: return structured failure result
            failure = FinalResult(
                final_answer="Cannot synthesize final result: no subtasks completed successfully.",
                supporting_context=contexts,
            )
            return failure.model_dump_json()
        # LLM-based synthesis
        if self.agent_llm:
            try:
                synth_prompt = (
                    "Phase: Synthesize Final Result\n"
                    "You are the Team Lead orchestrator. Given the successful subtasks, produce a final consolidated result. "
                    "Output ONLY a JSON object matching the FinalResult schema with fields:\n"
                    "- final_answer (string)\n"
                    "- supporting_context (array of strings)\n"
                    "Return the object fenced in ```json ... ```.\n"
                    f"Contexts: {contexts}\n"
                )
                # Debug log the constructed synthesis prompt
                logger.debug(f"_synthesize_final_result prompt:\n{synth_prompt}")
                synth_resp = await self.agent_llm.generate(synth_prompt)
                # Debug log the raw synthesis response
                logger.debug(f"_synthesize_final_result raw response:\n{synth_resp}")
                parsed_final = extract_json(synth_resp)
                # Debug log the parsed JSON from synthesis
                logger.debug(f"_synthesize_final_result parsed JSON: {parsed_final}")
                if not parsed_final:
                    raise ValueError("No JSON extracted from LLM synthesis response")
                final = FinalResult.model_validate(parsed_final)
                return final.model_dump_json()
            except Exception as e:
                logger.error(f"Error synthesizing final result: {e}")
                # Return structured failure result
                failure = FinalResult(
                    final_answer=f"Synthesis error: {e}", supporting_context=contexts
                )
                return failure.model_dump_json()
        # Fallback when no LLM available
        failure = FinalResult(
            final_answer="Cannot synthesize final result without LLM.",
            supporting_context=contexts,
        )
        return failure.model_dump_json()

    async def start(self, parent_task_id: str, goal_description: str) -> str:
        """Orchestrate the workflow with an asynchronous parallel-delegation state machine."""
        # --- Initialization ---
        self.state["parent_task_id"] = parent_task_id
        self.state["status"] = "analyzing"
        self.working_memory.add_entry(0, goal_description, "InitialGoal")
        self.persistent_memory.add_entry(
            {"turn": 0, "type": "goal", "content": goal_description}
        )

        # Phase: Decompose goal (only once)
        self.orchestration_state = OrchestrationState.PLANNING
        if not self.planned:
            try:
                subtasks = await self._decompose_goal(parent_task_id, goal_description)
            except Exception as e:
                # Planning failed: return a structured failure final result
                logger.error(f"Planning failed: {e}")
                failure = FinalResult(
                    final_answer=f"Planning error: {e}",
                    supporting_context=[goal_description],
                )
                return failure.model_dump_json()
            self.planned = True
            self.subtasks = {sub.id: sub for sub in subtasks}
            # Debug log the decomposed subtasks and their dependencies
            logger.debug(
                f"TeamLeadAgentLoop: Decomposed subtasks: {[{'id': sub.id, 'dependencies': sub.dependencies} for sub in subtasks]}"
            )
        else:
            subtasks = list(self.subtasks.values())
        # Initialize each TaskRecord for structured state tracking
        for sub in subtasks:
            self.task_states[sub.id] = TaskRecord(
                description=sub.description,
                dependencies=sub.dependencies,
                state=TaskStatus.PENDING.value,
                retries=0,
            )

        # Transition to delegation phase
        self.orchestration_state = OrchestrationState.READY_TO_DELEGATE
        awaiting = {
            record.task_id
            for record in self.task_states.values()
            if record.state == TaskStatus.ASSIGNED.value and record.task_id
        }

        # Main orchestration loop
        while not self.terminated:
            if self.orchestration_state == OrchestrationState.READY_TO_DELEGATE:
                # Debug log current completed subtasks and full task state map
                logger.debug(
                    f"TeamLeadAgentLoop: Completed_subtasks: {self.completed_subtasks}"
                )
                # Log TaskStates details as a dictionary to avoid f-string parsing issues
                states_details = {
                    sid: {"state": rec.state, "deps": rec.dependencies}
                    for sid, rec in self.task_states.items()
                }
                logger.debug(f"TeamLeadAgentLoop: TaskStates details: {states_details}")
                # Debug: Log eligible subtasks for delegation
                eligible = [
                    sub_id
                    for sub_id, rec in self.task_states.items()
                    if rec.state == TaskStatus.PENDING.value
                    and all(dep in self.completed_subtasks for dep in rec.dependencies)
                ]
                logger.debug(
                    f"TeamLeadAgentLoop: READY_TO_DELEGATE, eligible subtasks: {eligible}"
                )
                # Delegate all ready subtasks
                await self._delegate_ready_subtasks()
                # Update awaiting set of task IDs
                awaiting = {
                    record.task_id
                    for record in self.task_states.values()
                    if record.state == TaskStatus.ASSIGNED.value and record.task_id
                }
                if awaiting:
                    # Debug: Entering WAITING_FOR_REPORTS
                    logger.debug(
                        f"TeamLeadAgentLoop: Transitioning to WAITING_FOR_REPORTS for tasks: {list(awaiting)}"
                    )
                    self.orchestration_state = OrchestrationState.WAITING_FOR_REPORTS
                    self.state["status"] = f"waiting_reports:{list(awaiting)}"
                else:
                    logger.debug(
                        "TeamLeadAgentLoop: No assigned tasks pending, transitioning to READY_TO_SYNTHESIZE"
                    )
                    self.orchestration_state = OrchestrationState.READY_TO_SYNTHESIZE
                continue

            if self.orchestration_state == OrchestrationState.WAITING_FOR_REPORTS:
                # Debug: Waiting for reports on tasks
                logger.debug(
                    f"TeamLeadAgentLoop: WAITING_FOR_REPORTS, awaiting tasks: {awaiting}"
                )
                # Wait for a task_completed event from any member via the team's message queue
                for task_id in list(awaiting):
                    logger.debug(
                        f"TeamLeadAgentLoop: Waiting for report on task_id: {task_id}"
                    )
                    await self._wait_for_report(task_id)
                    logger.debug(
                        f"TeamLeadAgentLoop: Received report for task_id: {task_id}"
                    )
                    # After evaluation, update completed_subtasks if successful
                    try:
                        sub_id = next(
                            k for k, v in self.task_id_map.items() if v == task_id
                        )
                        if self.task_states[sub_id].state == TaskStatus.COMPLETE.value:
                            self.completed_subtasks.add(sub_id)
                    except StopIteration:
                        logger.warning(f"Received report for unknown task_id {task_id}")
                    awaiting.discard(task_id)
                    break
                # Debug: Transition back to delegation phase
                logger.debug(
                    "TeamLeadAgentLoop: Transitioning back to READY_TO_DELEGATE"
                )
                self.orchestration_state = OrchestrationState.READY_TO_DELEGATE
                continue

            if self.orchestration_state == OrchestrationState.READY_TO_SYNTHESIZE:
                break

        # Synthesize and return final result
        self.orchestration_state = OrchestrationState.READY_TO_SYNTHESIZE
        result_json = await self._synthesize_final_result()
        self.orchestration_state = OrchestrationState.COMPLETED
        self.working_memory.clear()
        return result_json

    def terminate(self, reason: Optional[str] = None):
        self.terminated = True
        self.termination_reason = reason

    def _select_least_busy_agent(self) -> Optional[str]:
        """Select the available agent with the fewest currently assigned tasks."""
        members = self.team.config.members
        if not members:
            return None
        # Count active assignments per agent
        counts = {m: 0 for m in members}
        for sub_id, agent in self.subtask_agent_map.items():
            record = self.task_states.get(sub_id)
            if record and record.state == TaskStatus.ASSIGNED.value:
                counts[agent] = counts.get(agent, 0) + 1
        # Find agent(s) with minimum count
        min_count = min(counts.values())
        for m in members:
            if counts.get(m, 0) == min_count:
                return m
        return None
