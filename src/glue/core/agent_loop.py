"""
Agent Loop module for the GLUE framework.

"""

from typing import List, Dict, Any, Callable, Tuple, Optional, TYPE_CHECKING, Set
import json
from .working_memory import WorkingMemory, PersistentMemory
from .schemas import ParseAnalyzeOutput, PlanPhaseOutput, ToolSelectionOutput, MemoryDecisionOutput, SelfEvalOutput, FormatResultOutput
from .orchestrator_schemas import Subtask, ReportRecord, EvaluateDecision, FinalResult, TaskRecord, DecomposeOutput
from .teams import Team
from ..utils.json_utils import extract_json
import asyncio
from .types import TaskStatus, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT
import logging
from datetime import datetime, timedelta
from uuid import uuid4
import re
from enum import Enum
from pydantic import BaseModel, Field, ValidationError, model_validator, ConfigDict
from typing import Literal

class LLMReportParams(BaseModel):
    """Validate parameters generated by the LLM for report_task_completion"""
    @model_validator(mode="before")
    def normalize_status(cls, data):
        # Map status 'completed' to 'success'
        status = data.get('status')
        if status == 'completed':
            data['status'] = 'success'
        return data
    status: Literal["success", "failure", "escalation"] = Field(..., description="Completion status from LLM")
    detailed_answer: str = Field(..., min_length=1, description="Detailed answer from LLM")
    artifact_keys: List[str] = Field(default_factory=list, description="Optional artifact keys from LLM")
    failure_reason: Optional[str] = Field(None, description="Optional failure reason from LLM")
    model_config = ConfigDict(extra="ignore")

# Enum to track TeamLead orchestration workflow state
class OrchestrationState(Enum):
    PLANNING = "PLANNING"
    READY_TO_DELEGATE = "READY_TO_DELEGATE"
    WAITING_FOR_REPORTS = "WAITING_FOR_REPORTS"
    READY_TO_SYNTHESIZE = "READY_TO_SYNTHESIZE"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

logger = logging.getLogger(__name__)

# Regex to identify delegated task IDs: e.g., "teamname-task-<8 hex chars>"
TASK_ID_REGEX = re.compile(r'^[a-z0-9\-]+\-task\-[0-9a-f]{8}$')

if TYPE_CHECKING:
    from .teams import Team

class TeamMemberAgentLoop:
    """loop for Team Member agents. Tools given are report_task_completion tool."""
    # Maximum number of times to retry LLM formatting when generating final JSON
    FORMAT_RESULT_MAX_RETRIES = 1

    def __init__(self, agent_id: str, team_id: str, report_tool: callable, agent_llm: Optional[Any] = None):
        self.agent_id = agent_id
        self.team_id = team_id
        # Track the current task ID explicitly to avoid stale state
        self.current_task_id = None
        # Store and wrap report_tool to auto-inject task_id from loop state
        raw_report_tool = report_tool
        def auto_report_tool(*, status, detailed_answer, artifact_keys=None, failure_reason=None, **_ignored):
            """Validate LLM-generated parameters and inject fixed context."""
            # Validate only the fields LLM is responsible for
            try:
                llm_params = LLMReportParams(
                    status=status,
                    detailed_answer=detailed_answer,
                    artifact_keys=artifact_keys or [],
                    failure_reason=failure_reason
                )
            except ValidationError as e:
                logger.error(f"LLMReportParams validation error: {e}")
                raise
            # Call the underlying tool with injected context
            result = raw_report_tool(
                task_id=self.state['current_task_id'],
                status=llm_params.status,
                detailed_answer=llm_params.detailed_answer,
                artifact_keys=llm_params.artifact_keys,
                calling_agent_id=self.agent_id,
                calling_team=self.team_id
            )
            return result
        self.report_tool = auto_report_tool
        # LLM interface for analysis and planning
        self.agent_llm = agent_llm
        # Registry for external tools
        self.tools: Dict[str, Callable] = {}
        # Loop termination state
        self.terminated: bool = False
        self.termination_reason: Optional[str] = None
        # Tracking for refine/revise logic
        self.last_substep_description: Optional[str] = None
        self.last_context_info: Optional[Dict[str, Any]] = None
        self.last_self_eval: Optional[SelfEvalOutput] = None
        self.last_tool_name: Optional[str] = None
        self.last_tool_params: Optional[Dict[str, Any]] = None
        self.state = {'current_task_id': None, 'status': 'idle'}
        # Initialize working memory and control counters
        self.working_memory = WorkingMemory()
        self.turn_counter: int = 0
        self.max_attempts: int = 3  # cap on loop iterations
        self.substeps: List[str] = []  # ordered list of substeps from plan
        self.current_step: int = 0  # index of the current substep being executed
        self.attempts_per_step: Dict[int, int] = {}  # track number of attempts per substep

    async def start(self, fetch_task_func: callable):
        """Start the Team Member loop to perform tasks and report completion."""
        logger.debug(f"TeamMemberAgentLoop({self.agent_id}): start invoked")

        # Continuously fetch and process tasks until explicitly terminated.
        while not self.terminated:
            # Fetch next task
            task = await fetch_task_func(self.agent_id)
            task_id = task.get('task_id')
            logger.debug(f"TeamMemberAgentLoop({self.agent_id}): fetched task {task_id}")
            logger.info(f"TeamMemberAgentLoop({self.agent_id}): begin processing task {task_id}")

            # Initialize state for new task
            self.current_task_id = task_id
            self.state['current_task_id'] = task_id
            self.state['status'] = 'working'

            # Reset execution context
            self.working_memory = WorkingMemory()
            self.turn_counter = 0
            self.substeps = []
            self.current_step = 0
            self.attempts_per_step = {}

            # Gather and analyze context
            self.gather_context(task)
            logger.debug(f"TeamMemberAgentLoop({self.agent_id}): initial context gathered for task {task_id}")
            analysis = await self.parse_and_analyze(task.get('description', ''))
            logger.debug(f"TeamMemberAgentLoop({self.agent_id}): parse_and_analyze result: {analysis}")
            self.working_memory.add_entry(self.turn_counter, str(analysis), 'ParseAnalyze')

            # Plan substeps
            plan_out = await self.plan_phase(self.working_memory.get_entries())
            logger.debug(f"TeamMemberAgentLoop({self.agent_id}): plan_phase result: substeps={plan_out.substeps}, tools={plan_out.tool_requirements}, confidence={plan_out.estimated_confidence}")
            self.working_memory.add_entry(self.turn_counter, str(plan_out), 'PlanPhase')
            self.substeps = plan_out.substeps
            self.current_step = 0
            self.attempts_per_step = {}

            # Execute substeps
            while not self.terminated:
                self.turn_counter += 1
                entries = self.working_memory.get_entries()
                recent = [e for e in entries if e.get('source_tool') != 'Curated'][-self.max_attempts:]
                context_info = {'turn': self.turn_counter, 'memory_entries': entries, 'recent_raw_outputs': recent}

                # Debug log execution stats
                logger.debug(f"Execution stats - turn_counter: {self.turn_counter}, current_step: {self.current_step}, attempts_per_step: {self.attempts_per_step}")
                desc = self.substeps[self.current_step]
                # Info log the substep execution for tracing
                logger.info(f"Task {self.current_task_id}: executing substep {self.current_step+1}/{len(self.substeps)}: '{desc}'")
                logger.debug(f"TeamMemberAgentLoop({self.agent_id}): executing substep {self.current_step} '{desc}' with context={context_info}")
                tool_name, result = await self.select_and_invoke_tool(desc, context_info)
                logger.debug(f"TeamMemberAgentLoop({self.agent_id}): tool '{tool_name}' returned result: {result}")

                # Save and optionally curate memory
                self.working_memory.add_entry(self.turn_counter, str(result), tool_name)
                mem_dec = await self.should_save_to_memory(result)
                if mem_dec.save_to_memory:
                    self.working_memory.add_entry(self.turn_counter, mem_dec.analysis or '', 'Curated')

                # Self evaluation
                eval_out = await self.evaluate_self(result)
                self.last_self_eval = eval_out
                confidence = eval_out.confidence_level.lower()
                # Debug log the self-evaluation confidence
                logger.debug(f"self-evaluation confidence: {confidence}")
                self.attempts_per_step[self.current_step] = self.attempts_per_step.get(self.current_step, 0) + 1

                if confidence.startswith('high'):
                    # Next substep or finish
                    if self.current_step < len(self.substeps) - 1:
                        self.current_step += 1
                        continue

                    # Report success
                    answer = await self.format_result(result)
                    status = 'success' if result.get('success') else 'failure'
                    logger.debug(f"Preparing to report task completion for {task_id}: status={status}")
                    await self.report_tool(status=status, detailed_answer=answer, artifact_keys=result.get('artifacts', []))
                    self.terminate(f"Task {task_id} completed")
                    self.state['status'] = 'completed'
                    break
                elif confidence.startswith('medium'):
                    result = await self.refine_result(result)
                    continue
                elif confidence.startswith('low'):
                    if self.attempts_per_step[self.current_step] < self.max_attempts:
                        result = await self.revise_strategy(result)
                        continue

                    # Report failure
                    answer = await self.format_result(result)
                    logger.debug(f"Preparing to report failure for task {task_id}: status=failure")
                    await self.report_tool(status='failure', detailed_answer=answer, artifact_keys=result.get('artifacts', []), failure_reason=result.get('error'))
                    self.terminate(f"Task {task_id} failed")
                    self.state['status'] = 'completed'
                    break
                else:
                    # Critical error
                    answer = await self.format_result(result)
                    logger.debug(f"Preparing to report critical error for task {task_id}")
                    await self.report_tool(status='failure', detailed_answer=answer, artifact_keys=[], failure_reason=result.get('error'))
                    self.terminate(f"Task {task_id} critical error")
                    self.state['status'] = 'completed'
                    break
        # End of persistent task loop.

    async def performTaskStub(self, task_description: str) -> dict:
        """Placeholder for performing the task (simulated)."""
        import asyncio
        # Simulate work
        await asyncio.sleep(1)
        # Hardcoded success result with placeholder artifacts
        return {'success': True, 'artifacts': []}

    def generateSummaryStub(self, result: dict) -> str:
        """Placeholder for generating a detailed answer based on the result."""
        # Provide details including success status and any artifacts or errors
        if result.get('success'):
            artifacts = result.get('artifacts', [])
            return f"Task completed successfully with artifacts: {artifacts}" 
        else:
            # Include any error message if present
            error_msg = result.get('error', 'Unknown error')
            return f"Task failed: {error_msg}"

    def gather_context(self, task: Dict[str, Any]):
        """Collect initial context from the task description."""
        self.working_memory.add_entry(self.turn_counter, task['description'], 'TaskReceived')

    async def parse_and_analyze(self, description: str) -> ParseAnalyzeOutput:
        """Use LLM to extract high-level requirements from task description as structured JSON."""
        # Trace parse_and_analyze invocation
        logger.info(f"Task {self.current_task_id}: parse_and_analyze starting for description: {description}")
        if not self.agent_llm:
            # Fallback stub without LLM
            keywords = description.split()[:3]
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"keywords": keywords}
            )
        # Construct prompt requesting structured JSON output per schema
        prompt = (
            "Provide ONLY a fenced JSON response matching the ParseAnalyzeOutput schema,\n"
            "with fields:\n"
            "- thought_process (string)\n"
            "- analysis (object)\n"
            "Example:\n```json\n"
            "{\"thought_process\":\"parsed reasoning\",\"analysis\":{\"key\":\"value\"}}\n"
            "```\n"
            "Now analyze the following task description:\n"
            f"{description}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"parse_and_analyze prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"parse_and_analyze raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if parsed:
            try:
                return ParseAnalyzeOutput.parse_obj(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return ParseAnalyzeOutput.parse_obj(data)
        except Exception:
            # Final stub fallback
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"analysis": response}
            )

    async def plan_phase(self, memory: List[Dict[str, Any]]) -> PlanPhaseOutput:
        """Use LLM to break memory entries into substeps, tool requirements, and estimated confidence as structured JSON."""
        # Trace plan_phase invocation
        logger.info(f"Task {self.current_task_id}: plan_phase starting with memory entries count: {len(memory)}")
        if not self.agent_llm:
            # Fallback stub without LLM
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high"
            )
        # Prompt LLM for structured JSON output per schema
        prompt = (
            "Provide ONLY a fenced JSON response matching the PlanPhaseOutput schema,\n"
            "with fields:\n"
            "- substeps (array of strings)\n"
            "- tool_requirements (array of strings)\n"
            "- estimated_confidence (string)\n"
            "Example:\n```json\n"
            "{\"substeps\":[\"Step 1\",\"Step 2\"],\"tool_requirements\":[\"tool1\"],\"estimated_confidence\":\"high\"}\n"
            "```\n"
            "Plan execution for the following memory entries:\n"
            f"{json.dumps(memory)}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"plan_phase prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"plan_phase raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return PlanPhaseOutput.parse_obj(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return PlanPhaseOutput.parse_obj(data)
        except Exception:
            # Final stub fallback
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high"
            )

    async def select_and_invoke_tool(self, task_description: str, context_info: Optional[Dict[str, Any]] = None) -> Tuple[str, Any]:
        """Use LLM to select the next tool and invoke it based on structured JSON output."""
        # Info log tool selection start
        logger.info(f"Task {self.current_task_id}: select_and_invoke_tool for substep '{task_description}'")
        # Track substep and context
        self.last_substep_description = task_description
        self.last_context_info = context_info
        if not self.agent_llm:
            # Fallback stub execution
            self.last_tool_name = 'performTaskStub'
            self.last_tool_params = {'task_description': task_description}
            result = await self.performTaskStub(task_description)
            return 'performTaskStub', result
        # Prompt LLM for tool selection JSON, including memory context if available
        prompt = (
            "Provide ONLY a fenced JSON response matching the ToolSelectionOutput schema,\n"
            "with fields:\n"
            "- selected_tool_name (string)\n"
            "- tool_parameters (object)\n"
            "Example:\n```json\n"
            "{\"selected_tool_name\":\"performTask\",\"tool_parameters\":{\"param\":\"value\"}}\n"
            "```\n"
            "Choose the next tool for the following substep and context.\n"
            f"Substep: {task_description}\n"
        )
        if context_info is not None:
            prompt += f"Context: {json.dumps(context_info)}"
        # Debug log the constructed prompt
        logger.debug(f"select_and_invoke_tool prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"select_and_invoke_tool raw response:\n{response}")
        parsed = extract_json(response)
        # Info log parsed selection JSON
        logger.info(f"Task {self.current_task_id}: select_and_invoke_tool parsed JSON: {parsed}")
        selection = None
        if isinstance(parsed, dict):
            try:
                selection = ToolSelectionOutput.parse_obj(parsed)
                # Info log the selected tool and parameters
                logger.info(f"Task {self.current_task_id}: selected tool '{selection.selected_tool_name}' with params {selection.tool_parameters}")
            except Exception:
                selection = None
        # Invoke selected tool or fallback
        if selection:
            tool_name = selection.selected_tool_name
            params = selection.tool_parameters or {}
            # Track tool invocation
            self.last_tool_name = tool_name
            self.last_tool_params = params
            # Intercept internal report: use auto_report_tool to inject task_id
            if tool_name == 'report_task_completion':
                # Debug log interception and params
                logger.debug(f"Intercepting report_task_completion for task {self.current_task_id} with params {params}")
                result = await self.report_tool(**params)
                # Debug log the result of tool invocation
                logger.debug(f"select_and_invoke_tool report_task_completion result: {result}")
                return tool_name, result
            # First try internal method
            if hasattr(self, tool_name) and callable(getattr(self, tool_name)):
                try:
                    result = await getattr(self, tool_name)(**params)
                    # Debug log the result of internal tool invocation
                    logger.debug(f"select_and_invoke_tool internal tool {tool_name} result: {result}")
                    return tool_name, result
                except Exception:
                    pass
            # Then try external tool registry
            if tool_name in self.tools and callable(self.tools[tool_name]):
                try:
                    result = await self.tools[tool_name](**params)
                    # Debug log the result of external tool invocation
                    logger.debug(f"select_and_invoke_tool external tool {tool_name} result: {result}")
                    return tool_name, result
                except Exception:
                    pass
        # Fallback to performTaskStub on failure
        result = await self.performTaskStub(task_description)
        return 'performTaskStub', result

    async def should_save_to_memory(self, result: Any) -> MemoryDecisionOutput:
        """Prompt agent to decide if result should be saved using the MemoryDecisionOutput schema."""
        if not self.agent_llm:
            return MemoryDecisionOutput(save_to_memory=False)
        prompt = (
            "Provide ONLY a fenced JSON response matching the MemoryDecisionOutput schema,\n"
            "with fields:\n"
            "- save_to_memory (boolean)\n"
            "- analysis (string)\n"
            "Example:\n```json\n"
            "{\"save_to_memory\":true,\"analysis\":\"key details\"}\n"
            "```\n"
            "Decide whether to save the following tool result:\n"
            f"{result}\n"
        )
        response = await self.agent_llm.generate(prompt)
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return MemoryDecisionOutput.parse_obj(parsed)
            except Exception:
                pass
        # Fallback stub
        return MemoryDecisionOutput(save_to_memory=False)

    async def evaluate_self(self, result: Any) -> SelfEvalOutput:
        """Use LLM to self-evaluate and return a SelfEvalOutput object."""
        if not self.agent_llm:
            # Default stub: always high confidence for fallback
            stub = SelfEvalOutput(
                evaluation_summary="Fallback stub self-evaluation",
                consistency_check="Passed",
                alignment_check="Passed",
                confidence_level="high",
                error_detected=False
            )
            return stub
        # Construct prompt for SelfEvalOutput
        prompt = (
            "Please output ONLY valid JSON matching the SelfEvalOutput schema "
            "(fields: evaluation_summary, consistency_check, alignment_check, confidence_level, error_detected) "
            "for the following result and context entries:\n"
            f"Result: {result}\n"
            f"Context: {json.dumps(self.working_memory.get_entries())}"
        )
        # Debug log the constructed prompt
        logger.debug(f"evaluate_self prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"evaluate_self raw response:\n{response}")
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return SelfEvalOutput.parse_obj(parsed)
            except Exception:
                pass
        # Fallback stub if parsing failed
        stub = SelfEvalOutput(
            evaluation_summary="Fallback stub self-evaluation",
            consistency_check="Passed",
            alignment_check="Passed",
            confidence_level="HighConfidence",
            error_detected=False
        )
        return stub

    async def refine_result(self, last_result: Any) -> Any:
        """Refine the last result by re-invoking the substep tool with feedback."""
        # Gather feedback from last self-evaluation
        feedback = self.last_self_eval.evaluation_summary if self.last_self_eval else ''
        # Debug log the feedback used for refinement
        logger.debug(f"refine_result feedback: {feedback}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context['refine_feedback'] = feedback
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"refine_result context: {context}")
        # Re-invoke tool for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from refine_result
        logger.debug(f"refine_result new result: {new_result}")
        return new_result

    async def revise_strategy(self, last_result: Any) -> Any:
        """Revise strategy for the current substep by planning a new approach."""
        # Gather failure reason from last self-evaluation
        reason = self.last_self_eval.evaluation_summary if self.last_self_eval else ''
        # Debug log the failure reason for revision
        logger.debug(f"revise_strategy failure reason: {reason}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context['failure_reason'] = reason
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"revise_strategy context: {context}")
        # Re-invoke planning/execution for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from revise_strategy
        logger.debug(f"revise_strategy new result: {new_result}")
        return new_result

    async def format_result(self, result: Any) -> str:
        """Use LLM to format the final result as JSON matching the FormatResultOutput schema."""
        entries = self.working_memory.get_entries()
        # If no LLM available, return fallback JSON immediately
        if not self.agent_llm:
            fmt = FormatResultOutput(
                final_answer=str(result),
                supporting_context=[str(e) for e in entries]
            )
            return fmt.json()
        # Simplified prompt for reliable JSON output
        prompt = (
            "Return a JSON object with exactly two keys: \"final_answer\" (string) and \"supporting_context\" (array of strings).\n"
            f"Result: {result}\n"
            f"Context entries: {json.dumps(entries)}"
        )
        last_response = None
        last_error = None
        # Retry loop for LLM formatting
        for attempt in range(self.FORMAT_RESULT_MAX_RETRIES + 1):
            response = await self.agent_llm.generate(prompt)
            last_response = response
            logger.debug(f"format_result attempt {attempt} raw response: {response}")
            parsed = extract_json(response)
            if isinstance(parsed, dict):
                try:
                    fmt_out = FormatResultOutput.parse_obj(parsed)
                    return fmt_out.json()
                except Exception as e:
                    last_error = e
                    logger.debug(f"format_result parse error: {e}")
                    continue
            last_error = Exception("Failed to extract JSON from response")
        # Fallback after retries: include error details in supporting_context
        error_desc = str(last_error) if last_error else "Unknown error"
        fallback_context = [str(e) for e in entries] + [f"format_result_error: {error_desc}", f"last_response: {last_response}"]
        fmt = FormatResultOutput(
            final_answer=str(result),
            supporting_context=fallback_context
        )
        return fmt.json()

    def register_tool(self, name: str, func: Callable):
        """Register an external tool callable under the given name."""
        self.tools[name] = func

    def terminate(self, reason: Optional[str] = None):
        """Externally terminate the loop execution."""
        self.terminated = True
        self.termination_reason = reason

class TeamLeadAgentLoop:
    """Full orchestrator loop for Team Lead agents."""
    def __init__(self, team: Team, delegate_tool: Callable, agent_llm: Optional[Any] = None):
        self.team = team
        self.delegate_tool = delegate_tool
        self.agent_llm = agent_llm
        self.agent_id = team.config.lead
        self.state = {'parent_task_id': None, 'status': 'idle'}
        self.working_memory = WorkingMemory()
        self.persistent_memory = PersistentMemory(self.team.name)
        # Track each subtask with a structured Pydantic model
        self.task_states: Dict[str, TaskRecord] = {}
        self.terminated = False
        self.termination_reason: Optional[str] = None
        # Configure maximum orchestration retries per subtask
        self.max_retries = int(DEFAULT_MAX_RETRIES)
        self.timeout = DEFAULT_TIMEOUT  # NEW: set default timeout
        # Initialize orchestration state and tracking structures
        self.orchestration_state: OrchestrationState = OrchestrationState.PLANNING
        self.subtasks: Dict[str, Subtask] = {}
        self.task_id_map: Dict[str, str] = {}
        self.completed_subtasks: Set[str] = set()
        self.planned: bool = False

    async def _decompose_goal(self, parent_task_id: str, goal_description: str) -> List[Subtask]:
        """Use LLM to decompose goal into subtasks and log the JSON to working_memory."""
        if not self.agent_llm:
            raise RuntimeError("TeamLeadAgentLoop: Missing LLM for decomposition")
        prompt = (
            "Phase: Decompose Goal\n"
            "You are the Team Lead orchestrator. Given a high-level goal, break it into an ordered list of subtasks. "
            "Output ONLY a JSON array of objects, each with fields:\n"
            "- id (string): unique subtask identifier\n"
            "- description (string): subtask description\n"
            "- dependencies (array of strings): list of prerequisite subtask IDs (empty list if none)\n"
            "Return the array fenced in ```json ... ```.\n"
            f"High-level goal: {goal_description}\n"
        )
        try:
            response = await self.agent_llm.generate(prompt)
            parsed = extract_json(response)
            if not parsed or not isinstance(parsed, list):
                raise ValueError(f"Failed to parse subtasks from LLM response: {response}")
            subtasks = [Subtask.parse_obj(item) for item in parsed]
        except Exception as e:
            logger.error(f"TeamLeadAgentLoop._decompose_goal: {e}")
            # Abort orchestration on decomposition failure
            raise RuntimeError(f"Decomposition error: {e}")
        # Record decomposition output as structured dicts
        parsed_subtasks = [s.dict() for s in subtasks]
        decomposition_json = {"tool_name": "decompose_goal", "arguments": {"subtasks": parsed_subtasks, "parent_task_id": parent_task_id}}
        self.working_memory.add_entry(1, json.dumps(decomposition_json), "OrchestratorOutput")
        return subtasks

    async def _delegate_pending_subtasks(self) -> None:
        """Delegate any subtasks in PENDING or PENDING_RETRY state."""
        members = self.team.config.members
        if not members:
            logger.error("No members available to delegate pending subtasks")
            for sub_id, info in self.task_states.items():
                if info.get('state') in (TaskStatus.PENDING, TaskStatus.PENDING_RETRY):
                    info['state'] = TaskStatus.NO_MEMBER
            self.terminated = True
            return
        for sub_id, info in list(self.task_states.items()):
            if info.get('state') in (TaskStatus.PENDING, TaskStatus.PENDING_RETRY):
                target = members[0]
                delegate_json = {
                    'tool_name': 'delegate_task',
                    'arguments': {
                        'target_agent_id': target,
                        'parent_task_id': self.state['parent_task_id'],
                        'task_description': info.get('description'),
                        'context_keys': [],
                        'required_artifacts': []
                    }
                }
                self.working_memory.add_entry(0, json.dumps(delegate_json), 'OrchestratorOutput')
                result = await self.delegate_tool(
                    target_agent_id=target,
                    task_description=info.get('description'),
                    parent_task_id=self.state['parent_task_id'],
                    calling_agent_id=self.agent_id,
                    calling_team=self.team.name
                )
                info['task_id'] = result.get('task', {}).get('task_id')
                self.task_id_map[sub_id] = info['task_id']
                info['state'] = TaskStatus.ASSIGNED
                info['timestamp'] = datetime.utcnow()

    async def _delegate_ready_subtasks(self) -> None:
        """Delegate subtasks whose dependencies are met and are pending."""
        members = self.team.config.members
        if not members:
            logger.error("No members available to delegate ready subtasks")
            return
        for sub_id, subtask in self.subtasks.items():
            state_info = self.task_states.get(sub_id)
            if state_info and state_info.state == TaskStatus.PENDING.value and all(
                dep in self.completed_subtasks for dep in state_info.dependencies
            ):
                target = members[0]
                delegate_json = {
                    'tool_name': 'delegate_task',
                    'arguments': {
                        'target_agent_id': target,
                        'parent_task_id': self.state['parent_task_id'],
                        'task_description': subtask.description,
                        'context_keys': [],
                        'required_artifacts': []
                    }
                }
                self.working_memory.add_entry(0, json.dumps(delegate_json), 'OrchestratorOutput')
                result = await self.delegate_tool(
                    target_agent_id=target,
                    task_description=subtask.description,
                    parent_task_id=self.state['parent_task_id'],
                    calling_agent_id=self.agent_id,
                    calling_team=self.team.name
                )
                task_id = result.get('task', {}).get('task_id')
                self.task_id_map[sub_id] = task_id
                state_info.task_id = task_id
                state_info.state = TaskStatus.ASSIGNED.value
                state_info.timestamp = datetime.utcnow().isoformat()

    async def _wait_for_report(self, task_id: str) -> None:
        """Wait for a single subtask completion report and evaluate it."""
        while True:
            entry = self.team.shared_results.get(task_id)
            if isinstance(entry, dict) and 'completion' in entry:
                report = ReportRecord.parse_obj(entry['completion'])
                await self._evaluate_report(report)
                break
            await asyncio.sleep(1)

    async def _evaluate_report(self, report: ReportRecord) -> None:
        """Evaluate a single report: log decision, update task_states, and handle retry or escalation."""
        # Map report status to orchestration action
        action_map = {'success': 'mark_complete', 'failure': 'retry', 'escalation': 'escalate'}
        # Include failure_reason if provided, otherwise use detailed_answer
        detail_text = report.failure_reason if report.failure_reason else report.detailed_answer
        decision = EvaluateDecision(
            task_id=report.task_id,
            action=action_map.get(report.status),
            details=detail_text
        )
        # Log evaluation JSON
        eval_json = {'tool_name': 'evaluate_result', 'arguments': decision.dict()}
        self.working_memory.add_entry(0, json.dumps(eval_json), 'EvaluateResult')
        # Find corresponding task state
        entry = next((record for record in self.task_states.values() if record.task_id == report.task_id), None)
        if not entry or entry.state in (TaskStatus.COMPLETE.value, TaskStatus.FAILED.value):
            return
        entry.state = TaskStatus.REPORTED.value
        # Execute action
        if decision.action == 'mark_complete':
            entry.state = TaskStatus.COMPLETE.value
        elif decision.action == 'retry':
            if entry.retries < self.max_retries:
                entry.retries += 1
                entry.state = TaskStatus.PENDING_RETRY.value
            else:
                entry.state = TaskStatus.FAILED.value
                self.terminated = True
        else:  # escalate
            entry.state = TaskStatus.FAILED.value
            self.terminated = True

    async def _synthesize_final_result(self) -> str:
        """Synthesize final result via LLM or fallback and return FinalResult JSON."""
        # Update status
        self.state['status'] = 'synthesizing'
        # Gather context contents
        contexts = [e['content'] for e in self.working_memory.get_entries()]
        # Pre-check: ensure at least one subtask completed successfully
        successful = [rec for rec in self.task_states.values() if rec.state == TaskStatus.COMPLETE.value]
        if not successful:
            # No successful subtasks: return structured failure result
            failure = FinalResult(
                final_answer="Cannot synthesize final result: no subtasks completed successfully.",
                supporting_context=contexts
            )
            return failure.json()
        # LLM-based synthesis
        if self.agent_llm:
            try:
                synth_prompt = (
                    "Phase: Synthesize Final Result\n"
                    "You are the Team Lead orchestrator. Given the successful subtasks, produce a final consolidated result. "
                    "Output ONLY a JSON object matching the FinalResult schema with fields:\n"
                    "- final_answer (string)\n"
                    "- supporting_context (array of strings)\n"
                    "Return the object fenced in ```json ... ```.\n"
                    f"Contexts: {contexts}\n"
                )
                # Debug log the constructed synthesis prompt
                logger.debug(f"_synthesize_final_result prompt:\n{synth_prompt}")
                synth_resp = await self.agent_llm.generate(synth_prompt)
                # Debug log the raw synthesis response
                logger.debug(f"_synthesize_final_result raw response:\n{synth_resp}")
                parsed_final = extract_json(synth_resp)
                # Debug log the parsed JSON from synthesis
                logger.debug(f"_synthesize_final_result parsed JSON: {parsed_final}")
                if not parsed_final:
                    raise ValueError("No JSON extracted from LLM synthesis response")
                final = FinalResult.parse_obj(parsed_final)
                return final.json()
            except Exception as e:
                logger.error(f"Error synthesizing final result: {e}")
                # Return structured failure result
                failure = FinalResult(
                    final_answer=f"Synthesis error: {e}",
                    supporting_context=contexts
                )
                return failure.json()
        # Fallback when no LLM available
        failure = FinalResult(
            final_answer="Cannot synthesize final result without LLM.",
            supporting_context=contexts
        )
        return failure.json()

    async def start(self, parent_task_id: str, goal_description: str) -> str:
        """Orchestrate the workflow with an asynchronous parallel-delegation state machine."""
        # --- Initialization ---
        self.state['parent_task_id'] = parent_task_id
        self.state['status'] = 'analyzing'
        self.working_memory.add_entry(0, goal_description, 'InitialGoal')
        self.persistent_memory.add_entry({'turn': 0, 'type': 'goal', 'content': goal_description})

        # Phase: Decompose goal (only once)
        self.orchestration_state = OrchestrationState.PLANNING
        if not self.planned:
            try:
                subtasks = await self._decompose_goal(parent_task_id, goal_description)
            except Exception as e:
                # Planning failed: return a structured failure final result
                logger.error(f"Planning failed: {e}")
                failure = FinalResult(
                    final_answer=f"Planning error: {e}",
                    supporting_context=[goal_description]
                )
                return failure.json()
            self.planned = True
            self.subtasks = {sub.id: sub for sub in subtasks}
            # Debug log the decomposed subtasks and their dependencies
            logger.debug(f"TeamLeadAgentLoop: Decomposed subtasks: {[{'id': sub.id, 'dependencies': sub.dependencies} for sub in subtasks]}")
        else:
            subtasks = list(self.subtasks.values())
        # Initialize each TaskRecord for structured state tracking
        for sub in subtasks:
            self.task_states[sub.id] = TaskRecord(
                description=sub.description,
                dependencies=sub.dependencies,
                state=TaskStatus.PENDING.value,
                retries=0,
            )

        # Transition to delegation phase
        self.orchestration_state = OrchestrationState.READY_TO_DELEGATE
        awaiting = {
            record.task_id
            for record in self.task_states.values()
            if record.state == TaskStatus.ASSIGNED.value and record.task_id
        }

        # Main orchestration loop
        while not self.terminated:
            if self.orchestration_state == OrchestrationState.READY_TO_DELEGATE:
                # Debug log current completed subtasks and full task state map
                logger.debug(f"TeamLeadAgentLoop: Completed_subtasks: {self.completed_subtasks}")
                logger.debug(f"TeamLeadAgentLoop: TaskStates details: {{ {', '.join([f"{sid}:{{state:{rec.state},deps:{rec.dependencies}}}" for sid, rec in self.task_states.items()])} }}")
                # Debug: Log eligible subtasks for delegation
                eligible = [sub_id for sub_id, rec in self.task_states.items() if rec.state == TaskStatus.PENDING.value and all(
                    dep in self.completed_subtasks for dep in rec.dependencies
                )]
                logger.debug(f"TeamLeadAgentLoop: READY_TO_DELEGATE, eligible subtasks: {eligible}")
                # Delegate all ready subtasks
                await self._delegate_ready_subtasks()
                # Update awaiting set of task IDs
                awaiting = {
                    record.task_id
                    for record in self.task_states.values()
                    if record.state == TaskStatus.ASSIGNED.value and record.task_id
                }
                if awaiting:
                    # Debug: Entering WAITING_FOR_REPORTS
                    logger.debug(f"TeamLeadAgentLoop: Transitioning to WAITING_FOR_REPORTS for tasks: {list(awaiting)}")
                    self.orchestration_state = OrchestrationState.WAITING_FOR_REPORTS
                    self.state['status'] = f"waiting_reports:{list(awaiting)}"
                else:
                    logger.debug("TeamLeadAgentLoop: No assigned tasks pending, transitioning to READY_TO_SYNTHESIZE")
                    self.orchestration_state = OrchestrationState.READY_TO_SYNTHESIZE
                continue

            if self.orchestration_state == OrchestrationState.WAITING_FOR_REPORTS:
                # Debug: Waiting for reports on tasks
                logger.debug(f"TeamLeadAgentLoop: WAITING_FOR_REPORTS, awaiting tasks: {awaiting}")
                # Wait for a task_completed event from any member via the team's message queue
                for task_id in list(awaiting):
                    logger.debug(f"TeamLeadAgentLoop: Waiting for report on task_id: {task_id}")
                    await self._wait_for_report(task_id)
                    logger.debug(f"TeamLeadAgentLoop: Received report for task_id: {task_id}")
                    # After evaluation, update completed_subtasks if successful
                    try:
                        sub_id = next(k for k, v in self.task_id_map.items() if v == task_id)
                        if self.task_states[sub_id].state == TaskStatus.COMPLETE.value:
                            self.completed_subtasks.add(sub_id)
                    except StopIteration:
                        logger.warning(f"Received report for unknown task_id {task_id}")
                    awaiting.discard(task_id)
                    break
                # Debug: Transition back to delegation phase
                logger.debug("TeamLeadAgentLoop: Transitioning back to READY_TO_DELEGATE")
                self.orchestration_state = OrchestrationState.READY_TO_DELEGATE
                continue

            if self.orchestration_state == OrchestrationState.READY_TO_SYNTHESIZE:
                break

        # Synthesize and return final result
        self.orchestration_state = OrchestrationState.READY_TO_SYNTHESIZE
        result_json = await self._synthesize_final_result()
        self.orchestration_state = OrchestrationState.COMPLETED
        self.working_memory.clear()
        return result_json

    def terminate(self, reason: Optional[str] = None):
        self.terminated = True
        self.termination_reason = reason
