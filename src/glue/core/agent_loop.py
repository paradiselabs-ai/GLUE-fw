"""
Agent Loop module for the GLUE framework.

"""

from typing import List, Dict, Any, Callable, Tuple, Optional, TYPE_CHECKING, Set
import json
from .working_memory import WorkingMemory, PersistentMemory
from .schemas import (
    ParseAnalyzeOutput,
    PlanPhaseOutput,
    ToolSelectionOutput,
    MemoryDecisionOutput,
    SelfEvalOutput,
    FormatResultOutput,
)
from .orchestrator_schemas import (
    Subtask,
    ReportRecord,
    EvaluateDecision,
    FinalResult,
    TaskRecord,
)
from .teams import Team
from ..utils.json_utils import extract_json
import asyncio
from .types import TaskStatus, DEFAULT_MAX_RETRIES, DEFAULT_TIMEOUT
import logging
from datetime import datetime
import re
from enum import Enum
from pydantic import BaseModel, Field, ValidationError, model_validator, ConfigDict
from typing import Literal


class LLMReportParams(BaseModel):
    """Validate parameters generated by the LLM for report_task_completion"""

    @model_validator(mode="before")
    def normalize_status(cls, data):
        # Map status 'completed' to 'success'
        status = data.get("status")
        if status == "completed":
            data["status"] = "success"
        return data

    status: Literal["success", "failure", "escalation"] = Field(
        ..., description="Completion status from LLM"
    )
    detailed_answer: str = Field(
        ..., min_length=1, description="Detailed answer from LLM"
    )
    artifact_keys: List[str] = Field(
        default_factory=list, description="Optional artifact keys from LLM"
    )
    failure_reason: Optional[str] = Field(
        None, description="Optional failure reason from LLM"
    )
    model_config = ConfigDict(extra="ignore")


# Enum to track TeamLead orchestration workflow state
class OrchestrationState(Enum):
    PLANNING = "PLANNING"
    READY_TO_DELEGATE = "READY_TO_DELEGATE"
    WAITING_FOR_REPORTS = "WAITING_FOR_REPORTS"
    READY_TO_SYNTHESIZE = "READY_TO_SYNTHESIZE"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"


logger = logging.getLogger(__name__)

# Regex to identify delegated task IDs: e.g., "teamname-task-<8 hex chars>"
TASK_ID_REGEX = re.compile(r"^[a-z0-9\-]+\-task\-[0-9a-f]{8}$")

if TYPE_CHECKING:
    # Type checking for Team models imported above
    pass


# Deprecated: Use GlueSmolTeam (src/glue/core/glue_smolteam.py) and GlueSmolAgent integration instead.
class TeamLeadAgentLoop:
    def __init__(self, *args, **kwargs):
        raise NotImplementedError(
            "TeamLeadAgentLoop is deprecated; use GlueSmolTeam instead."
        )


# Deprecated: Use GlueSmolAgent logic for member task loops.
class TeamMemberAgentLoop:
    def __init__(self, *args, **kwargs):
        raise NotImplementedError(
            "TeamMemberAgentLoop is deprecated; use GlueSmolAgent instead."
        )


class TeamMemberAgentLoop:
    """loop for Team Member agents. Tools given are report_task_completion tool."""

    # Maximum number of times to retry LLM formatting when generating final JSON
    FORMAT_RESULT_MAX_RETRIES = 1

    def __init__(
        self,
        agent_id: str,
        team_id: str,
        report_tool: callable,
        agent_llm: Optional[Any] = None,
    ):
        self.agent_id = agent_id
        self.team_id = team_id
        # Track the current task ID explicitly to avoid stale state
        self.current_task_id = None
        # Store and wrap report_tool to auto-inject task_id from loop state
        raw_report_tool = report_tool

        def auto_report_tool(
            *,
            status,
            detailed_answer,
            artifact_keys=None,
            failure_reason=None,
            **_ignored,
        ):
            """Validate LLM-generated parameters and inject fixed context."""
            # Validate only the fields LLM is responsible for
            try:
                llm_params = LLMReportParams(
                    status=status,
                    detailed_answer=detailed_answer,
                    artifact_keys=artifact_keys or [],
                    failure_reason=failure_reason,
                )
            except ValidationError as e:
                logger.error(f"LLMReportParams validation error: {e}")
                raise
            # Call the underlying tool with injected context
            result = raw_report_tool(
                task_id=self.state["current_task_id"],
                status=llm_params.status,
                detailed_answer=llm_params.detailed_answer,
                artifact_keys=llm_params.artifact_keys,
                calling_agent_id=self.agent_id,
                calling_team=self.team_id,
            )
            return result

        self.report_tool = auto_report_tool
        # LLM interface for analysis and planning
        self.agent_llm = agent_llm
        # Registry for external tools
        self.tools: Dict[str, Callable] = {}
        # Loop termination state
        self.terminated: bool = False
        self.termination_reason: Optional[str] = None
        # Tracking for refine/revise logic
        self.last_substep_description: Optional[str] = None
        self.last_context_info: Optional[Dict[str, Any]] = None
        self.last_self_eval: Optional[SelfEvalOutput] = None
        self.last_tool_name: Optional[str] = None
        self.last_tool_params: Optional[Dict[str, Any]] = None
        self.state = {"current_task_id": None, "status": "idle"}
        # Initialize working memory and control counters
        self.working_memory = WorkingMemory()
        self.turn_counter: int = 0
        self.max_attempts: int = 3  # cap on loop iterations
        self.substeps: List[str] = []  # ordered list of substeps from plan
        self.current_step: int = 0  # index of the current substep being executed
        self.attempts_per_step: Dict[
            int, int
        ] = {}  # track number of attempts per substep

    async def start(self, fetch_task_func: callable):
        """Start the Team Member loop to perform tasks and report completion."""
        logger.debug(f"TeamMemberAgentLoop({self.agent_id}): start invoked")

        # Continuously fetch and process tasks until explicitly terminated.
        while not self.terminated:
            # Fetch next task
            task = await fetch_task_func(self.agent_id)
            # Debug log the full task received to inspect context_keys and prior data
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): received task dict: {task}"
            )
            task_id = task.get("task_id")
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): fetched task {task_id}"
            )
            logger.info(
                f"TeamMemberAgentLoop({self.agent_id}): begin processing task {task_id}"
            )

            # Initialize state for new task
            self.current_task_id = task_id
            self.state["current_task_id"] = task_id
            self.state["status"] = "working"

            # Reset execution context
            self.working_memory = WorkingMemory()
            self.turn_counter = 0
            self.substeps = []
            self.current_step = 0
            self.attempts_per_step = {}

            # Gather and analyze context
            self.gather_context(task)
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): initial context gathered for task {task_id}"
            )
            analysis = await self.parse_and_analyze(task.get("description", ""))
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): parse_and_analyze result: {analysis}"
            )
            self.working_memory.add_entry(
                self.turn_counter, str(analysis), "ParseAnalyze"
            )

            # Plan substeps
            plan_out = await self.plan_phase(self.working_memory.get_entries())
            logger.debug(
                f"TeamMemberAgentLoop({self.agent_id}): plan_phase result: substeps={plan_out.substeps}, tools={plan_out.tool_requirements}, confidence={plan_out.estimated_confidence}"
            )
            self.working_memory.add_entry(self.turn_counter, str(plan_out), "PlanPhase")
            self.substeps = plan_out.substeps
            self.current_step = 0
            self.attempts_per_step = {}

            # Execute substeps
            while not self.terminated:
                self.turn_counter += 1
                entries = self.working_memory.get_entries()
                recent = [e for e in entries if e.get("source_tool") != "Curated"][
                    -self.max_attempts :
                ]
                context_info = {
                    "turn": self.turn_counter,
                    "memory_entries": entries,
                    "recent_raw_outputs": recent,
                }

                # Debug log execution stats
                logger.debug(
                    f"Execution stats - turn_counter: {self.turn_counter}, current_step: {self.current_step}, attempts_per_step: {self.attempts_per_step}"
                )
                desc = self.substeps[self.current_step]
                # Info log the substep execution for tracing
                logger.info(
                    f"Task {self.current_task_id}: executing substep {self.current_step + 1}/{len(self.substeps)}: '{desc}'"
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): executing substep {self.current_step} '{desc}' with context={context_info}"
                )
                tool_name, result = await self.select_and_invoke_tool(
                    desc, context_info
                )
                logger.debug(
                    f"TeamMemberAgentLoop({self.agent_id}): tool '{tool_name}' returned result: {result}"
                )

                # Save and optionally curate memory
                self.working_memory.add_entry(self.turn_counter, str(result), tool_name)
                mem_dec = await self.should_save_to_memory(result)
                if mem_dec.save_to_memory:
                    self.working_memory.add_entry(
                        self.turn_counter, mem_dec.analysis or "", "Curated"
                    )

                # Self evaluation
                eval_out = await self.evaluate_self(result)
                self.last_self_eval = eval_out
                confidence = eval_out.confidence_level.lower()
                # Debug log the self-evaluation confidence
                logger.debug(f"self-evaluation confidence: {confidence}")
                self.attempts_per_step[self.current_step] = (
                    self.attempts_per_step.get(self.current_step, 0) + 1
                )

                if confidence.startswith("high"):
                    # Next substep or finish
                    if self.current_step < len(self.substeps) - 1:
                        self.current_step += 1
                        continue

                    # Report success
                    answer = await self.format_result(result)
                    status = "success" if result.get("success") else "failure"
                    logger.debug(
                        f"Preparing to report task completion for {task_id}: status={status}"
                    )
                    await self.report_tool(
                        status=status,
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                    )
                    self.terminate(f"Task {task_id} completed")
                    self.state["status"] = "completed"
                    break
                elif confidence.startswith("medium"):
                    result = await self.refine_result(result)
                    continue
                elif confidence.startswith("low"):
                    if self.attempts_per_step[self.current_step] < self.max_attempts:
                        result = await self.revise_strategy(result)
                        continue

                    # Report failure
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report failure for task {task_id}: status=failure"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=result.get("artifacts", []),
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} failed")
                    self.state["status"] = "completed"
                    break
                else:
                    # Critical error
                    answer = await self.format_result(result)
                    logger.debug(
                        f"Preparing to report critical error for task {task_id}"
                    )
                    await self.report_tool(
                        status="failure",
                        detailed_answer=answer,
                        artifact_keys=[],
                        failure_reason=result.get("error"),
                    )
                    self.terminate(f"Task {task_id} critical error")
                    self.state["status"] = "completed"
                    break
        # End of persistent task loop.

    async def performTaskStub(self, task_description: str) -> dict:
        """Placeholder for performing the task (simulated)."""
        import asyncio

        # Simulate work
        await asyncio.sleep(1)
        # Hardcoded success result with placeholder artifacts
        return {"success": True, "artifacts": []}

    def generateSummaryStub(self, result: dict) -> str:
        """Placeholder for generating a detailed answer based on the result."""
        # Provide details including success status and any artifacts or errors
        if result.get("success"):
            artifacts = result.get("artifacts", [])
            return f"Task completed successfully with artifacts: {artifacts}"
        else:
            # Include any error message if present
            error_msg = result.get("error", "Unknown error")
            return f"Task failed: {error_msg}"

    def gather_context(self, task: Dict[str, Any]):
        """Collect initial context from the task description."""
        # Log context_keys and required_artifacts to verify context passing
        logger.debug(
            f"TeamMemberAgentLoop({self.agent_id}): gather_context for task {task.get('task_id')}, "
            f"description={task.get('description')}, context_keys={task.get('context_keys')}, required_artifacts={task.get('required_artifacts')}"
        )
        # Add the description to working memory
        self.working_memory.add_entry(
            self.turn_counter, task["description"], "TaskReceived"
        )

    async def parse_and_analyze(self, description: str) -> ParseAnalyzeOutput:
        """Use LLM to extract high-level requirements from task description as structured JSON."""
        # Trace parse_and_analyze invocation
        logger.info(
            f"Task {self.current_task_id}: parse_and_analyze starting for description: {description}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            keywords = description.split()[:3]
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"keywords": keywords},
            )
        # Construct prompt requesting detailed structured JSON output per schema with examples
        prompt = (
            "Phase: Parse and Analyze Task\n"
            "You are a reasoning agent tasked with deeply analyzing the following task description. "
            "Produce a detailed, step-by-step `thought_process` explaining how you interpreted and decomposed the task, and a structured `analysis` object capturing core requirements, subcomponents, and any constraints. "
            "Return ONLY a fenced JSON object matching the ParseAnalyzeOutput schema with fields:\n"
            "- thought_process (string): detailed reasoning steps\n"
            "- analysis (object): key-value mapping for identified elements or requirements\n"
            "\n"
            "Few-shot examples:\n"
            'Input: "Filter a list to only include even numbers and then sort it."\n'
            "Output:\n```json\n"
            "{\n"
            '  "thought_process": "First identify the actions (filter then sort), determine filter criterion (even numbers), apply filter, then apply sort.",\n'
            '  "analysis": {\n'
            '    "steps": ["filter even numbers", "sort list"],\n'
            '    "filter_criterion": "even numbers",\n'
            '    "operation_order": ["filter", "sort"]\n'
            "  }\n"
            "}\n```\n"
            'Input: "Compute the intersection subset of two user ID sets."\n'
            "Output:\n```json\n"
            "{\n"
            '  "thought_process": "Recognize the need for a set intersection, identify both input sets, and compute their common elements. The term \'subset\' indicates selecting elements shared by both.",\n'
            '  "analysis": {\n'
            '    "operation": "intersection",\n'
            '    "inputs": ["set A", "set B"],\n'
            '    "subset_criteria": "common elements"\n'
            "  }\n"
            "}\n```\n"
            "Now analyze the following task description:\n"
            f"{description}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"parse_and_analyze prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"parse_and_analyze raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if parsed:
            try:
                return ParseAnalyzeOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return ParseAnalyzeOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return ParseAnalyzeOutput(
                thought_process="Fallback stub analysis",
                analysis={"analysis": response},
            )

    async def plan_phase(self, memory: List[Dict[str, Any]]) -> PlanPhaseOutput:
        """Use LLM to break memory entries into substeps, tool requirements, and estimated confidence as structured JSON."""
        # Trace plan_phase invocation
        logger.info(
            f"Task {self.current_task_id}: plan_phase starting with memory entries count: {len(memory)}"
        )
        if not self.agent_llm:
            # Fallback stub without LLM
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )
        # Construct an enriched planning prompt guiding the LLM to produce clear substeps, tool requirements, and confidence
        prompt = (
            "Phase: Plan Task Execution\n"
            "You are a planning agent. Given the following memory entries, break down the upcoming task into an ordered list of actionable substeps. "
            "Specify any required tools for each step and provide an overall confidence rating (low, medium, high). "
            "Return ONLY a fenced JSON object matching the PlanPhaseOutput schema with keys:\n"
            "- substeps: array of step descriptions\n"
            "- tool_requirements: array of tool names needed\n"
            "- estimated_confidence: confidence level as 'low', 'medium', or 'high'\n"
            "\n"
            "Few-shot examples:\n"
            'Input memory entries: ["Convert text to lowercase and count word frequency."]\n'
            "Output:\n```json\n"
            '{"substeps":["lowercase text","count word frequencies"],"tool_requirements":["textProcessor"],"estimated_confidence":"high"}\n'
            "```\n"
            'Input memory entries: ["Fetch user data from API, filter active users, then export as CSV."]\n'
            "Output:\n```json\n"
            '{"substeps":["fetch user data via API","filter active users","export data to CSV"],"tool_requirements":["apiClient","csvExporter"],"estimated_confidence":"medium"}\n'
            "```\n"
            "Now plan execution for the following memory entries:\n"
            f"{json.dumps(memory)}\n"
        )
        # Debug log the constructed prompt
        logger.debug(f"plan_phase prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"plan_phase raw response:\n{response}")
        # Attempt to extract JSON from response
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return PlanPhaseOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback to direct JSON loads
        try:
            data = json.loads(response)
            return PlanPhaseOutput.model_validate(data)
        except Exception:
            # Final stub fallback
            return PlanPhaseOutput(
                substeps=["Step 1", "Step 2", "Step 3"],
                tool_requirements=[],
                estimated_confidence="high",
            )

    async def select_and_invoke_tool(
        self, task_description: str, context_info: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, Any]:
        """Use LLM to select the next tool and invoke it based on structured JSON output."""
        # Info log tool selection start
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool for substep '{task_description}'"
        )
        # Track substep and context
        self.last_substep_description = task_description
        self.last_context_info = context_info
        if not self.agent_llm:
            # Fallback stub execution
            self.last_tool_name = "performTaskStub"
            self.last_tool_params = {"task_description": task_description}
            result = await self.performTaskStub(task_description)
            return "performTaskStub", result
        # Construct enriched tool selection prompt with few-shot examples and schema guidance
        prompt = (
            "Phase: Select and Invoke Tool\n"
            "You are an intelligent agent responsible for selecting the correct tool for the following substep. "
            "Refer to the available tools and their expected parameter schemas. "
            "Return ONLY a JSON object matching the ToolSelectionOutput schema with keys:\n"
            "- selected_tool_name (string): the tool name to invoke\n"
            "- tool_parameters (object): key-value mapping of parameters for the selected tool\n"
            "\n"
            "Few-shot examples:\n"
            "Example 1:\n"
            'Substep: "Parse and analyze description"\n'
            'Context: {"description": "..."}\n'
            "Output:\n```json\n"
            '{"selected_tool_name":"parse_and_analyze","tool_parameters":{"description":"..."}}\n'
            "```\n"
            "Example 2:\n"
            'Substep: "Report task completion"\n'
            'Context: {"status":"success","detailed_answer":"..."}\n'
            "Output:\n```json\n"
            '{"selected_tool_name":"report_task_completion","tool_parameters":{"status":"success","detailed_answer":"..."}}\n'
            "```\n"
            f"Substep: {task_description}\n"
        )
        if context_info is not None:
            prompt += f"Context: {json.dumps(context_info)}"
        # Debug log the constructed prompt
        logger.debug(f"select_and_invoke_tool prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"select_and_invoke_tool raw response:\n{response}")
        parsed = extract_json(response)
        # Info log parsed selection JSON
        logger.info(
            f"Task {self.current_task_id}: select_and_invoke_tool parsed JSON: {parsed}"
        )
        selection = None
        if isinstance(parsed, dict):
            try:
                selection = ToolSelectionOutput.model_validate(parsed)
                # Info log the selected tool and parameters
                logger.info(
                    f"Task {self.current_task_id}: selected tool '{selection.selected_tool_name}' with params {selection.tool_parameters}"
                )
            except Exception:
                selection = None
        # Invoke selected tool or fallback
        if selection:
            tool_name = selection.selected_tool_name
            params = selection.tool_parameters or {}
            # Track tool invocation
            self.last_tool_name = tool_name
            self.last_tool_params = params
            # Intercept internal report: use auto_report_tool to inject task_id
            if tool_name == "report_task_completion":
                # Debug log interception and params
                logger.debug(
                    f"Intercepting report_task_completion for task {self.current_task_id} with params {params}"
                )
                result = await self.report_tool(**params)
                # Debug log the result of tool invocation
                logger.debug(
                    f"select_and_invoke_tool report_task_completion result: {result}"
                )
                return tool_name, result
            # First try internal method
            if hasattr(self, tool_name) and callable(getattr(self, tool_name)):
                try:
                    result = await getattr(self, tool_name)(**params)
                    # Debug log the result of internal tool invocation
                    logger.debug(
                        f"select_and_invoke_tool internal tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
            # Then try external tool registry
            if tool_name in self.tools and callable(self.tools[tool_name]):
                try:
                    result = await self.tools[tool_name](**params)
                    # Debug log the result of external tool invocation
                    logger.debug(
                        f"select_and_invoke_tool external tool {tool_name} result: {result}"
                    )
                    return tool_name, result
                except Exception:
                    pass
        # Fallback to performTaskStub on failure
        result = await self.performTaskStub(task_description)
        return "performTaskStub", result

    async def should_save_to_memory(self, result: Any) -> MemoryDecisionOutput:
        """Prompt agent to decide if result should be saved using the MemoryDecisionOutput schema."""
        if not self.agent_llm:
            return MemoryDecisionOutput(save_to_memory=False)
        prompt = (
            "Provide ONLY a fenced JSON response matching the MemoryDecisionOutput schema,\n"
            "with fields:\n"
            "- save_to_memory (boolean)\n"
            "- analysis (string)\n"
            "Example:\n```json\n"
            '{"save_to_memory":true,"analysis":"key details"}\n'
            "```\n"
            "Decide whether to save the following tool result:\n"
            f"{result}\n"
        )
        response = await self.agent_llm.generate(prompt)
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return MemoryDecisionOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub
        return MemoryDecisionOutput(save_to_memory=False)

    async def evaluate_self(self, result: Any) -> SelfEvalOutput:
        """Use LLM to self-evaluate and return a SelfEvalOutput object."""
        if not self.agent_llm:
            # Default stub: always high confidence for fallback
            stub = SelfEvalOutput(
                evaluation_summary="Fallback stub self-evaluation",
                consistency_check="Passed",
                alignment_check="Passed",
                confidence_level="high",
                error_detected=False,
            )
            return stub
        # Construct prompt for SelfEvalOutput
        prompt = (
            "Please output ONLY valid JSON matching the SelfEvalOutput schema "
            "(fields: evaluation_summary, consistency_check, alignment_check, confidence_level, error_detected) "
            "for the following result and context entries:\n"
            f"Result: {result}\n"
            f"Context: {json.dumps(self.working_memory.get_entries())}"
        )
        # Debug log the constructed prompt
        logger.debug(f"evaluate_self prompt:\n{prompt}")
        # Invoke LLM generation
        response = await self.agent_llm.generate(prompt)
        # Debug log the raw LLM response
        logger.debug(f"evaluate_self raw response:\n{response}")
        parsed = extract_json(response)
        if isinstance(parsed, dict):
            try:
                return SelfEvalOutput.model_validate(parsed)
            except Exception:
                pass
        # Fallback stub if parsing failed
        stub = SelfEvalOutput(
            evaluation_summary="Fallback stub self-evaluation",
            consistency_check="Passed",
            alignment_check="Passed",
            confidence_level="HighConfidence",
            error_detected=False,
        )
        return stub

    async def refine_result(self, last_result: Any) -> Any:
        """Refine the last result by re-invoking the substep tool with feedback."""
        # Gather feedback from last self-evaluation
        feedback = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the feedback used for refinement
        logger.debug(f"refine_result feedback: {feedback}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["refine_feedback"] = feedback
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"refine_result context: {context}")
        # Re-invoke tool for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from refine_result
        logger.debug(f"refine_result new result: {new_result}")
        return new_result

    async def revise_strategy(self, last_result: Any) -> Any:
        """Revise strategy for the current substep by planning a new approach."""
        # Gather failure reason from last self-evaluation
        reason = self.last_self_eval.evaluation_summary if self.last_self_eval else ""
        # Debug log the failure reason for revision
        logger.debug(f"revise_strategy failure reason: {reason}")
        context = dict(self.last_context_info) if self.last_context_info else {}
        context["failure_reason"] = reason
        # Debug log the context being passed to select_and_invoke_tool
        logger.debug(f"revise_strategy context: {context}")
        # Re-invoke planning/execution for the same substep
        _, new_result = await self.select_and_invoke_tool(
            self.last_substep_description, context
        )
        # Debug log the new result from revise_strategy
        logger.debug(f"revise_strategy new result: {new_result}")
        return new_result

    async def format_result(self, result: Any) -> str:
        """Format the final result using the internal summary stub and working memory context."""
        # Gather all memory entries
        entries = self.working_memory.get_entries()
        # Use internal summary stub for final answer
        summary = self.generateSummaryStub(result)
        # Build supporting context from entries
        context = [str(e) for e in entries]
        # Create output object
        fmt = FormatResultOutput(final_answer=summary, supporting_context=context)
        return fmt.model_dump_json()

    def register_tool(self, name: str, func: Callable):
        """Register an external tool callable under the given name."""
        self.tools[name] = func

    def terminate(self, reason: Optional[str] = None):
        """Externally terminate the loop execution."""
        self.terminated = True
        self.termination_reason = reason
